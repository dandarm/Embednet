{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#per la riproducibilità\n",
    "%env CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import Markdown as md\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx import stochastic_block_model\n",
    "\n",
    "\n",
    "from graph_generation import GenerateGraph, perturb_nx_graph\n",
    "from models import GCN, view_parameters, new_parameters, modify_parameters, Inits, new_parameters_linears\n",
    "from train import Trainer, Dataset\n",
    "from embedding import Embedding\n",
    "from config_valid import Config, TrainingMode\n",
    "import experiments\n",
    "from experiments import Experiments, all_seeds\n",
    "from plot_funcs import (plot_dim1, plot_dimN, plot_correlation_error, plot_metrics, plot_node_emb_1D, plot_node_emb_nD, scatter_node_emb, \n",
    "                        plot_graph_emb_1D, plot_graph_emb_nD, plot_data_degree_sequence, plot_corr_epoch, plot_ripetizioni_stesso_trial, \n",
    "                        plot_onlyloss_ripetizioni_stesso_trial,plot_onlyloss_ripetizioni_stesso_trial_superimposed, Data2Plot, plot_weights_multiple_hist)\n",
    "from plot_model import plot_model\n",
    "from utils import array_wo_outliers, plot_grafo, plot_grafo2\n",
    "from Inspect import Inspect\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda')\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import pickle \n",
    "from plt_parameters import init_params, get_colors_to_cycle_rainbow8, get_colors_to_cycle_rainbowN\n",
    "init_params()\n",
    "rootsave = Path(\"output_plots/\")\n",
    "\n",
    "def make_video_parallel_static():\n",
    "    experiments.graph_embedding_per_epoch = xp.trainer.graph_embedding_per_epoch\n",
    "    experiments.node_embedding_per_epoch = xp.trainer.node_embedding_per_epoch\n",
    "    experiments.dataset = xp.trainer.dataset\n",
    "    experiments.loss_list = xp.trainer.test_loss_list\n",
    "    experiments.exp_config = xp.trainer.config_class\n",
    "    experiments.dataset_type = xp.trainer.gg.graphtype\n",
    "    num_emb_neurons = xp.trainer.model.convs[-1].out_channels\n",
    "    experiments.trainmode = xp.trainer.config_class.modo\n",
    "    #experiments.num_classes = xp.trainer.config_class.num_classes\n",
    "    experiments.embedding_dimension = num_emb_neurons\n",
    "    #my_list = my_log_lista=list(range(20)) + list(range(20,100,4)) + list(range(100,500, 15)) + list(range(500, 5000, 40))\n",
    "    nomefile = xp.make_video(skip=1, fromfiles=True, custom_list=True, seq_colors=True)\n",
    "    return nomefile\n",
    "\n",
    "\n",
    "def run_grid_w_gif(xp):\n",
    "    nomifilesgif = []\n",
    "    k = 0\n",
    "    for c in xp.gc.configs:  \n",
    "        print(f'Run {k + 1}\\t\\t exp name: {c.unique_train_name}')\n",
    "        # all_seeds()\n",
    "        xp.trainer.reinit_conf(c)\n",
    "        xp.just_train()\n",
    "        embedding_class = xp.embedding()\n",
    "        num_emb_neurons = xp.trainer.model.convs[-1].out_channels\n",
    "        trainmode = xp.trainer.config_class.modo\n",
    "        embedding_class.get_metrics(num_emb_neurons, trainmode)    \n",
    "        nomefile = make_video_parallel_static()\n",
    "        nomifilesgif.append(nomefile)\n",
    "        k += 1\n",
    "    return nomifilesgif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder model\n",
      "8 configurazioni saltate su 9, farò i seguenti 1 training:\n",
      "GraphType.CM_Classi2_nodi45_grafiXtipo40_mode1_layers§1-32-2§_initw-xavier_normal_lr0.001_GCNfreezedFalse\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configurations/Final1.yml\"\n",
    "num_nodi = 45\n",
    "c = Config(config_file)\n",
    "c.conf['graph_dataset']['list_exponents'] = [-1.9, -2.5]\n",
    "c.conf['model']['autoencoder'] = True\n",
    "diz_trials = {'graph_dataset.ERmodel': [False], 'graph_dataset.confmodel': [True], 'graph_dataset.sbm': [False],\n",
    "              'graph_dataset.Num_nodes': [num_nodi, [num_nodi]*7, [num_nodi]*2],  # per lo SBM: num nodi * num comunità \n",
    "              'model.GCNneurons_per_layer': [[1, 32, len(c.conf['graph_dataset']['list_exponents'])],\n",
    "                                            [1, 32, len(c.conf['graph_dataset']['list_p'])],\n",
    "                                            [1, 32, len(c.conf['graph_dataset']['community_probs'])],\n",
    "                                           ],\n",
    "             'model.init_weights': ['xavier_normal'],# 'eye'],\n",
    "             'model.freezeGCNlayers': [False],\n",
    "             'model.last_layer_dense': [False],\n",
    "             } \n",
    "\n",
    "xp = Experiments(config_file, diz_trials=diz_trials, rootsave=rootsave, config_class=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 \t\t exp name: GraphType.CM_Classi2_nodi45_grafiXtipo40_mode1_layers§1-32-2§_initw-xavier_normal_lr0.001_GCNfreezedFalse\n",
      "Generating dataset...\n",
      "Dataset generated\n",
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/80 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGS_simple_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/experiments.py:127\u001b[0m, in \u001b[0;36mExperiments.GS_simple_experiments\u001b[0;34m(self, list_points, parallel_take_result)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# all_seeds()\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mreinit_conf(c)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjust_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m embedding_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding()\n\u001b[1;32m    130\u001b[0m embedding_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mout_channels\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/experiments.py:395\u001b[0m, in \u001b[0;36mExperiments.just_train\u001b[0;34m(self, parallel, verbose)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjust_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlaunch_training()\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/train.py:181\u001b[0m, in \u001b[0;36mTrainer.init_all\u001b[0;34m(self, parallel, verbose)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_dataset(parallel\u001b[38;5;241m=\u001b[39mparallel, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# parallel false perché con load_from_networkx non c'è nulla da fare...\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    183\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msample_dummy_data()\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/train_autoencoder.py:51\u001b[0m, in \u001b[0;36mTrainer_Autoencoder.load_dataset\u001b[0;34m(self, dataset, parallel)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m DatasetAutoencoder\u001b[38;5;241m.\u001b[39mfrom_super_instance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpercentage_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_class, dataset)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/Dataset_autoencoder.py:58\u001b[0m, in \u001b[0;36mDatasetAutoencoder.prepare\u001b[0;34m(self, shuffle, parallel)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m     starttime \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_pyg, test_pyg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnx2pyg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     durata \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m starttime\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTempo impiegato: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdurata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documenti/Progetti/Networks/Embednet/Dataset_autoencoder.py:43\u001b[0m, in \u001b[0;36mDatasetAutoencoder.nx2pyg\u001b[0;34m(self, graph_list_nx, parallel)\u001b[0m\n\u001b[1;32m     41\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m tqdm(graph_list_nx, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(graph_list_nx)):\n\u001b[0;32m---> 43\u001b[0m     pyg_graph, test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_G_autoencoder((g, i))\n\u001b[1;32m     44\u001b[0m     dataset_pyg\u001b[38;5;241m.\u001b[39mappend(pyg_graph)\n\u001b[1;32m     45\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "xp.GS_simple_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.init_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come funziona il dataset per l'autoencoder -> split delle edges per la reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/anaconda3/envs/pytorch-geom/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      split_labels=True, add_negative_train_samples=False),\n",
    "])\n",
    "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=transform)\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 7740], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], pos_edge_label=[3870], pos_edge_label_index=[2, 3870])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 7740], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327], pos_edge_label=[227], pos_edge_label_index=[2, 227], neg_edge_label=[227], neg_edge_label_index=[2, 227])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ...,  True,  True,  True], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtest(z, data\u001b[38;5;241m.\u001b[39mpos_edge_label_index, data\u001b[38;5;241m.\u001b[39mneg_edge_label_index)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train()     \n\u001b[1;32m     22\u001b[0m     auc, ap \u001b[38;5;241m=\u001b[39m test(test_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    if args.variational:\n",
    "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    loss = train()     \n",
    "    auc, ap = test(test_data)\n",
    "    # un problema con le diverse chiamate di test è non poter combinare le auc ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(deprecated) Data(x, val_pos_edge_index, test_pos_edge_index, train_pos_edge_index, train_neg_adj_mask, val_neg_edge_index, test_neg_edge_index])\n",
    "Data(x, edge_index, train_mask, val_mask, test_mask, pos_edge_label, pos_edge_label_index)\n",
    "Data(x, edge_index, train_mask, val_mask, test_mask, pos_edge_label, pos_edge_label_index, neg_edge_label, neg_edge_label_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split alternativo con il dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train:  pos_edge_label, pos_edge_label_index\n",
    "test:   pos_edge_label, pos_edge_label_index, neg_edge_label, neg_edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_train = data.edge_index[:, train_mask]\n",
    "edge_attr_train = data.edge_index[train_mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Geometric",
   "language": "python",
   "name": "pytorch-geom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
