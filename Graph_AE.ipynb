{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255c05f6-896d-4df1-91f6-852156ee98ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "! git clone https://github.com/Pangyk/Graph_AE.git "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636527bd-f820-4b85-b1f1-8a2a2d4db2a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "! pip install pandas\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-1.13.0+cu117.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b802b1b-8f55-4694-8952-fce5ccc7cba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/workspace/Embednet/')\n",
    "sys.path.append('/workspace/Graph_AE/')\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30a400b-42f7-424c-9b18-76b57fa104b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#per la riproducibilità\n",
    "%env CUBLAS_WORKSPACE_CONFIG=\":4096:8\"\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from IPython.display import Markdown as md\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from networkx import stochastic_block_model\n",
    "\n",
    "import torch_geometric\n",
    "from graph_generation import GenerateGraph, perturb_nx_graph\n",
    "from models import GCN, view_parameters, new_parameters, modify_parameters, Inits, new_parameters_linears\n",
    "from train import Trainer, Dataset\n",
    "from embedding import Embedding\n",
    "from config_valid import Config, TrainingMode\n",
    "import experiments\n",
    "from experiments import Experiments, all_seeds\n",
    "from plot_funcs import (plot_dim1, plot_dimN, plot_correlation_error, plot_metrics, plot_node_emb_1D, plot_node_emb_nD, scatter_node_emb, \n",
    "                        plot_graph_emb_1D, plot_graph_emb_nD, plot_data_degree_sequence, plot_corr_epoch, plot_ripetizioni_stesso_trial, \n",
    "                        plot_onlyloss_ripetizioni_stesso_trial,plot_onlyloss_ripetizioni_stesso_trial_superimposed, Data2Plot, plot_weights_multiple_hist)\n",
    "from plot_model import plot_model\n",
    "from utils import array_wo_outliers, plot_grafo, plot_grafo2\n",
    "from Inspect import Inspect\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric import nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda')\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import pickle \n",
    "from plt_parameters import init_params, get_colors_to_cycle_rainbow8, get_colors_to_cycle_rainbowN\n",
    "init_params()\n",
    "rootsave = Path(\"output_plots/\")\n",
    "\n",
    "def make_video_parallel_static():\n",
    "    experiments.graph_embedding_per_epoch = xp.trainer.graph_embedding_per_epoch\n",
    "    experiments.node_embedding_per_epoch = xp.trainer.node_embedding_per_epoch\n",
    "    experiments.dataset = xp.trainer.dataset\n",
    "    experiments.loss_list = xp.trainer.test_loss_list\n",
    "    experiments.exp_config = xp.trainer.config_class\n",
    "    experiments.dataset_type = xp.trainer.gg.graphtype\n",
    "    num_emb_neurons = xp.trainer.model.convs[-1].out_channels\n",
    "    experiments.trainmode = xp.trainer.config_class.modo\n",
    "    #experiments.num_classes = xp.trainer.config_class.num_classes\n",
    "    experiments.embedding_dimension = num_emb_neurons\n",
    "    #my_list = my_log_lista=list(range(20)) + list(range(20,100,4)) + list(range(100,500, 15)) + list(range(500, 5000, 40))\n",
    "    nomefile = xp.make_video(skip=1, fromfiles=True, custom_list=True, seq_colors=True)\n",
    "    return nomefile\n",
    "\n",
    "\n",
    "def run_grid_w_gif(xp):\n",
    "    nomifilesgif = []\n",
    "    k = 0\n",
    "    for c in xp.gc.configs:  \n",
    "        print(f'Run {k + 1}\\t\\t exp name: {c.unique_train_name}')\n",
    "        # all_seeds()\n",
    "        xp.trainer.reinit_conf(c)\n",
    "        xp.just_train()\n",
    "        embedding_class = xp.embedding()\n",
    "        num_emb_neurons = xp.trainer.model.convs[-1].out_channels\n",
    "        trainmode = xp.trainer.config_class.modo\n",
    "        embedding_class.get_metrics(num_emb_neurons, trainmode)    \n",
    "        nomefile = make_video_parallel_static()\n",
    "        nomifilesgif.append(nomefile)\n",
    "        k += 1\n",
    "    return nomifilesgif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e959af55-7521-406e-a46a-44d67692bdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Embednet\n"
     ]
    }
   ],
   "source": [
    "%cd Embednet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5096dcc0-0799-4ba2-bd56-e4b1c13f1d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 configurazioni saltate su 3, farò i seguenti 1 training:\n",
      "GraphType.SBM_Classi3_nodi900_grafiXtipo250_mode1_layers§1-128-64§_initw-xavier_normal_lr0.002_GCNfreezedFalse\n"
     ]
    }
   ],
   "source": [
    "config_file = \"configurations/Final1.yml\"\n",
    "num_nodi = 900\n",
    "c = Config(config_file)\n",
    "c.conf['graph_dataset']['Num_nodes'] = num_nodi\n",
    "c.conf['graph_dataset']['list_exponents'] = [-2.5]\n",
    "c.conf['model']['autoencoder'] = False\n",
    "c.conf['model']['autoencoder_graph_ae'] = True\n",
    "diz_trials = {'graph_dataset.ERmodel': [False], 'graph_dataset.confmodel': [False], 'graph_dataset.sbm': [False], 'graph_dataset.sbm': [True],\n",
    "              'graph_dataset.Num_nodes': [[num_nodi], [num_nodi]*7, [num_nodi]*2],  # per lo SBM: num nodi * num comunità \n",
    "              'model.GCNneurons_per_layer': [#[1, 32, 16, len(c.conf['graph_dataset']['list_exponents'])],\n",
    "                                            #[1, 32, 16, len(c.conf['graph_dataset']['list_p'])],\n",
    "                                            #[1, 32, 16, len(c.conf['graph_dataset']['community_probs'])],\n",
    "                                            [1, 128, 64]\n",
    "                                           ],\n",
    "             'model.init_weights': ['xavier_normal'],# 'eye'],\n",
    "             'model.freezeGCNlayers': [False],\n",
    "             'model.last_layer_dense': [False],\n",
    "             } \n",
    "\n",
    "xp = Experiments(diz_trials=diz_trials, rootsave=rootsave, config_class=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a63a3629-ab2e-49ab-85f1-36c037ca3b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 \t\t exp name: GraphType.SBM_Classi3_nodi900_grafiXtipo250_mode1_layers§1-128-64§_initw-xavier_normal_lr0.002_GCNfreezedFalse\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGS_simple_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Embednet/experiments.py:133\u001b[0m, in \u001b[0;36mExperiments.GS_simple_experiments\u001b[0;34m(self, list_points, parallel_take_result)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# all_seeds()\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mreinit_conf(c)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjust_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m embedding_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding()\n\u001b[1;32m    136\u001b[0m embedding_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mout_channels\n",
      "File \u001b[0;32m/workspace/Embednet/experiments.py:401\u001b[0m, in \u001b[0;36mExperiments.just_train\u001b[0;34m(self, parallel, verbose)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjust_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlaunch_training()\n",
      "File \u001b[0;32m/workspace/Embednet/train.py:186\u001b[0m, in \u001b[0;36mTrainer.init_all\u001b[0;34m(self, parallel, verbose)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mInizializza modello e datasest\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m:param parallel:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m:param verbose: se True ritorna il plot object del model\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m init_weigths_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_class\u001b[38;5;241m.\u001b[39minit_weights_mode\n\u001b[0;32m--> 186\u001b[0m w \u001b[38;5;241m=\u001b[39m new_parameters(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_GCN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, init_weigths_method)\n\u001b[1;32m    187\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_GCN(init_weights_gcn\u001b[38;5;241m=\u001b[39mw, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(model)\n",
      "File \u001b[0;32m/workspace/Embednet/train_autoencoderMIAGAE.py:36\u001b[0m, in \u001b[0;36mTrainer_AutoencoderMIAGAE.init_GCN\u001b[0;34m(self, init_weights_gcn, init_weights_lin, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoencoderMIAGAE\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#if init_weights_gcn is not None:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#    modify_parameters(model, init_weights_gcn)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#if init_weights_lin is not None:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#    modify_parameters_linear(model, init_weights_lin)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Embednet/model_MIAGAE.py:41\u001b[0m, in \u001b[0;36mAutoencoderMIAGAE.__init__\u001b[0;34m(self, config_class, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m GCNneurons_per_layer \u001b[38;5;241m=\u001b[39m graph_ae_configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGCNneurons_per_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mMIAGAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGCNneurons_per_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomp_rate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGCNneurons_per_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/workspace/Graph_AE/classification/Graph_AE.py:29\u001b[0m, in \u001b[0;36mNet.__init__\u001b[0;34m(self, input_size, kernels, depth, rate, shapes, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m     pool \u001b[38;5;241m=\u001b[39m TopKPooling(shapes[i], rate[i])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_list\u001b[38;5;241m.\u001b[39mappend(pool)\n\u001b[0;32m---> 29\u001b[0m     conv \u001b[38;5;241m=\u001b[39m SGAT(size, shapes[i], \u001b[43mshapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_list\u001b[38;5;241m.\u001b[39mappend(conv)\n\u001b[1;32m     31\u001b[0m pool \u001b[38;5;241m=\u001b[39m TopKPooling(shapes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], rate[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "xp.GS_simple_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c396d93-6803-46d4-bce0-c15e0adf6441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061c133-f094-47ed-8b9f-0701e0309cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd690e9-e7e2-464e-a114-bd7e47577063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0b3e6-cd74-417a-b06d-8b36259d73bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f54f5-8fee-448e-974a-5180f1382c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9f8d8-1fb5-402e-9c3e-5e85a6a9887b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f97859-dfa5-44f9-9036-b3052723e626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454fb01-b3ad-4319-8978-b39632cfd4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eafd6d1-526e-4c2d-a559-c3da5036502e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " 'Embednet/',\n",
       " 'Graph_AE/',\n",
       " '/workspace/Embednet/',\n",
       " '/workspace/Graph_AE/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e869ae5-4b69-4d48-9bae-693ee7331150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb5c0012-0752-49ad-8708-28efe1d8352a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from Graph_AE.utils.CustomDataSet import SelectGraph\n",
    "from Graph_AE.utils.train_utils import train_cp\n",
    "import torch\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c239ca44-c76e-42c7-a7b9-52e742832c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(d='REDDIT-BINARY', m='MIAGAE', device='cuda', batch=512, e=1000, lr=0.001, model_dir='data/model/', n_train=1500, n_test=1000, k=2, depth=3, c_rate=0.8, shapes='64,64,64')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Global_Dict generator')\n",
    "parser.add_argument('--d', type=str, default='FRANKENSTEIN', help=\"dataset name\")\n",
    "parser.add_argument('--m', type=str, default='MIAGAE', help=\"model name\")\n",
    "parser.add_argument('--device', type=str, default='cuda', help=\"cuda / cpu\")\n",
    "parser.add_argument('--batch', type=int, default=512, help=\"batch size\")\n",
    "parser.add_argument('--e', type=int, default=100, help=\"number of epochs\")\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument('--model_dir', type=str, default=\"data/model/\", help=\"path to save model\")\n",
    "parser.add_argument('--n_train', type=int, default=3000, help=\"number of samples for train set\")\n",
    "parser.add_argument('--n_test', type=int, default=1000, help=\"number of samples for test set\")\n",
    "parser.add_argument('--k', type=int, default=2, help=\"number of kernels\")\n",
    "parser.add_argument('--depth', type=int, default=3, help=\"depth of encoder and decoder\")\n",
    "parser.add_argument('--c_rate', type=float, default=0.8, help=\"compression ratio for each layer of encoder\")\n",
    "parser.add_argument('--shapes', type=str, default=\"64,64,64\", help=\"shape of each layer in encoder\")\n",
    "#args = parser.parse_args()\n",
    "sys.argv = ['--d','REDDIT-BINARY', '--n_train', '1500', '--e', '1000']\n",
    "args = parser.parse_args(sys.argv)\n",
    "#args = parser.parse_args(['--d', 'COLORS-3'])\n",
    "#args = parser.parse_args([])\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe4068c-a031-414e-beed-3c668912bc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "num_epoch = args.e\n",
    "batch_size = args.batch\n",
    "\n",
    "SelectGraph.data_name = args.d\n",
    "data_set = SelectGraph('data/' + SelectGraph.data_name)\n",
    "\n",
    "#check sulle feature\n",
    "dataset_mod = []\n",
    "for d in data_set:\n",
    "    if d.x is None:\n",
    "        d.x = torch.ones([d.num_nodes], dtype=torch.float).unsqueeze(1) \n",
    "    dataset_mod.append(d)\n",
    "\n",
    "    \n",
    "train_set = DataLoader(dataset_mod[:args.n_train], batch_size=batch_size, shuffle=True)\n",
    "test_set = DataLoader(dataset_mod[args.n_train:args.n_train + args.n_test], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7010db8b-93eb-4e25-96d5-9177b987e4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 # data_set.num_features\n",
    "print(input_size)\n",
    "shapes = list(map(int, args.shapes.split(\",\")))\n",
    "if args.m == \"MIAGAE\":\n",
    "    from classification.Graph_AE import Net\n",
    "    model = Net(input_size, args.k, args.depth, [args.c_rate] * args.depth, shapes, device).to(device)\n",
    "elif args.m == \"UNet\":\n",
    "    from classification.UNet import Net\n",
    "    model = Net(input_size, args.depth, args.c_rate, shapes, device).to(device)\n",
    "elif args.m == \"Gpool\":\n",
    "    from classification.Gpool_model import Net\n",
    "    model = Net(input_size, args.depth, args.c_rate, shapes, device).to(device)\n",
    "elif args.m == \"SAGpool\":\n",
    "    from classification.SAG_model import Net\n",
    "    model = Net(input_size, args.depth, [args.c_rate] * args.depth, shapes, device).to(device)\n",
    "else:\n",
    "    print(\"model not found\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5288fe5-e7a0-4635-815d-20d061a07dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 64, 64]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120700d8-7b8d-48b2-9225-162c33a6ebbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 000\n",
      "Training Loss: 0.403175284465154\n",
      "Test Loss: 0.22122237086296082\n",
      "\n",
      "Epoch: 001\n",
      "Training Loss: 0.2639915496110916\n",
      "Test Loss: 0.23105472326278687\n",
      "\n",
      "Epoch: 002\n",
      "Training Loss: 0.2252595474322637\n",
      "Test Loss: 0.2047766000032425\n",
      "\n",
      "Epoch: 003\n",
      "Training Loss: 0.22267609337965646\n",
      "Test Loss: 0.18446126580238342\n",
      "\n",
      "Epoch: 004\n",
      "Training Loss: 0.19482248524824777\n",
      "Test Loss: 0.17595168948173523\n",
      "\n",
      "Epoch: 005\n",
      "Training Loss: 0.18529823422431946\n",
      "Test Loss: 0.1582685112953186\n",
      "\n",
      "Epoch: 006\n",
      "Training Loss: 0.16560098528862\n",
      "Test Loss: 0.13848711550235748\n",
      "\n",
      "Epoch: 007\n",
      "Training Loss: 0.15317592024803162\n",
      "Test Loss: 0.12547194957733154\n",
      "\n",
      "Epoch: 008\n",
      "Training Loss: 0.13939167062441507\n",
      "Test Loss: 0.11563915759325027\n",
      "\n",
      "Epoch: 009\n",
      "Training Loss: 0.12797783315181732\n",
      "Test Loss: 0.10195785015821457\n",
      "\n",
      "Epoch: 010\n",
      "Training Loss: 0.11605112999677658\n",
      "Test Loss: 0.09047925472259521\n",
      "\n",
      "Epoch: 011\n",
      "Training Loss: 0.10657144834597905\n",
      "Test Loss: 0.08201421052217484\n",
      "\n",
      "Epoch: 012\n",
      "Training Loss: 0.09915376702944438\n",
      "Test Loss: 0.0747780129313469\n",
      "\n",
      "Epoch: 013\n",
      "Training Loss: 0.09397480636835098\n",
      "Test Loss: 0.06844613701105118\n",
      "\n",
      "Epoch: 014\n",
      "Training Loss: 0.08935050666332245\n",
      "Test Loss: 0.06185377016663551\n",
      "\n",
      "Epoch: 015\n",
      "Training Loss: 0.08384661376476288\n",
      "Test Loss: 0.05354229733347893\n",
      "\n",
      "Epoch: 016\n",
      "Training Loss: 0.07743505885203679\n",
      "Test Loss: 0.04510674625635147\n",
      "\n",
      "Epoch: 017\n",
      "Training Loss: 0.07085807373126347\n",
      "Test Loss: 0.03741826117038727\n",
      "\n",
      "Epoch: 018\n",
      "Training Loss: 0.06503738462924957\n",
      "Test Loss: 0.031237410381436348\n",
      "\n",
      "Epoch: 019\n",
      "Training Loss: 0.06055000548561414\n",
      "Test Loss: 0.02956443838775158\n",
      "\n",
      "Epoch: 020\n",
      "Training Loss: 0.05812638128797213\n",
      "Test Loss: 0.029594670981168747\n",
      "\n",
      "Epoch: 021\n",
      "Training Loss: 0.056185329953829445\n",
      "Test Loss: 0.02752096951007843\n",
      "\n",
      "Epoch: 022\n",
      "Training Loss: 0.05372313658396403\n",
      "Test Loss: 0.024777432903647423\n",
      "\n",
      "Epoch: 023\n",
      "Training Loss: 0.05154516423741976\n",
      "Test Loss: 0.02312137372791767\n",
      "\n",
      "Epoch: 024\n",
      "Training Loss: 0.04998708392182986\n",
      "Test Loss: 0.021974090486764908\n",
      "\n",
      "Epoch: 025\n",
      "Training Loss: 0.04870000605781873\n",
      "Test Loss: 0.020951047539711\n",
      "\n",
      "Epoch: 026\n",
      "Training Loss: 0.04754098256429037\n",
      "Test Loss: 0.020150864496827126\n",
      "\n",
      "Epoch: 027\n",
      "Training Loss: 0.046319821228583656\n",
      "Test Loss: 0.019564496353268623\n",
      "\n",
      "Epoch: 028\n",
      "Training Loss: 0.045762392381827034\n",
      "Test Loss: 0.019313940778374672\n",
      "\n",
      "Epoch: 029\n",
      "Training Loss: 0.04492298016945521\n",
      "Test Loss: 0.018888702616095543\n",
      "\n",
      "Epoch: 030\n",
      "Training Loss: 0.04424788181980451\n",
      "Test Loss: 0.01842334121465683\n",
      "\n",
      "Epoch: 031\n",
      "Training Loss: 0.04380855585138003\n",
      "Test Loss: 0.017838429659605026\n",
      "\n",
      "Epoch: 032\n",
      "Training Loss: 0.04288960744937261\n",
      "Test Loss: 0.017085548490285873\n",
      "\n",
      "Epoch: 033\n",
      "Training Loss: 0.04220296194156011\n",
      "Test Loss: 0.016451649367809296\n",
      "\n",
      "Epoch: 034\n",
      "Training Loss: 0.04135081668694814\n",
      "Test Loss: 0.015869855880737305\n",
      "\n",
      "Epoch: 035\n",
      "Training Loss: 0.04054781546195348\n",
      "Test Loss: 0.01525421254336834\n",
      "\n",
      "Epoch: 036\n",
      "Training Loss: 0.03989642113447189\n",
      "Test Loss: 0.014640687964856625\n",
      "\n",
      "Epoch: 037\n",
      "Training Loss: 0.039131573090950646\n",
      "Test Loss: 0.014072739519178867\n",
      "\n",
      "Epoch: 038\n",
      "Training Loss: 0.03836796432733536\n",
      "Test Loss: 0.013555710203945637\n",
      "\n",
      "Epoch: 039\n",
      "Training Loss: 0.03763533880313238\n",
      "Test Loss: 0.012961296364665031\n",
      "\n",
      "Epoch: 040\n",
      "Training Loss: 0.03688719496130943\n",
      "Test Loss: 0.012358883395791054\n",
      "\n",
      "Epoch: 041\n",
      "Training Loss: 0.03604663163423538\n",
      "Test Loss: 0.011746736243367195\n",
      "\n",
      "Epoch: 042\n",
      "Training Loss: 0.03534763430555662\n",
      "Test Loss: 0.011167614720761776\n",
      "\n",
      "Epoch: 043\n",
      "Training Loss: 0.03463955968618393\n",
      "Test Loss: 0.010567118413746357\n",
      "\n",
      "Epoch: 044\n",
      "Training Loss: 0.0338297002017498\n",
      "Test Loss: 0.01001786533743143\n",
      "\n",
      "Epoch: 045\n",
      "Training Loss: 0.03315146639943123\n",
      "Test Loss: 0.009464203380048275\n",
      "\n",
      "Epoch: 046\n",
      "Training Loss: 0.03234132006764412\n",
      "Test Loss: 0.008932657539844513\n",
      "\n",
      "Epoch: 047\n",
      "Training Loss: 0.031717838098605476\n",
      "Test Loss: 0.008406058885157108\n",
      "\n",
      "Epoch: 048\n",
      "Training Loss: 0.031078428650895756\n",
      "Test Loss: 0.007960326038300991\n",
      "\n",
      "Epoch: 049\n",
      "Training Loss: 0.03041724053521951\n",
      "Test Loss: 0.007577033713459969\n",
      "\n",
      "Epoch: 050\n",
      "Training Loss: 0.02978581128021081\n",
      "Test Loss: 0.007228813599795103\n",
      "\n",
      "Epoch: 051\n",
      "Training Loss: 0.029261667281389236\n",
      "Test Loss: 0.006953693460673094\n",
      "\n",
      "Epoch: 052\n",
      "Training Loss: 0.02858365699648857\n",
      "Test Loss: 0.006717975251376629\n",
      "\n",
      "Epoch: 053\n",
      "Training Loss: 0.02809714836378892\n",
      "Test Loss: 0.006541688460856676\n",
      "\n",
      "Epoch: 054\n",
      "Training Loss: 0.02778743828336398\n",
      "Test Loss: 0.006411353126168251\n",
      "\n",
      "Epoch: 055\n",
      "Training Loss: 0.027414044986168545\n",
      "Test Loss: 0.00628885580226779\n",
      "\n",
      "Epoch: 056\n",
      "Training Loss: 0.027012454966704052\n",
      "Test Loss: 0.00613077636808157\n",
      "\n",
      "Epoch: 057\n",
      "Training Loss: 0.02660108357667923\n",
      "Test Loss: 0.005962634924799204\n",
      "\n",
      "Epoch: 058\n",
      "Training Loss: 0.026171610380212467\n",
      "Test Loss: 0.005772208794951439\n",
      "\n",
      "Epoch: 059\n",
      "Training Loss: 0.025847514470418293\n",
      "Test Loss: 0.005557597614824772\n",
      "\n",
      "Epoch: 060\n",
      "Training Loss: 0.02542279101908207\n",
      "Test Loss: 0.005342062097042799\n",
      "\n",
      "Epoch: 061\n",
      "Training Loss: 0.02502436066667239\n",
      "Test Loss: 0.0051544466987252235\n",
      "\n",
      "Epoch: 062\n",
      "Training Loss: 0.024626687169075012\n",
      "Test Loss: 0.004958454053848982\n",
      "\n",
      "Epoch: 063\n",
      "Training Loss: 0.024224406729141872\n",
      "Test Loss: 0.004822612274438143\n",
      "\n",
      "Epoch: 064\n",
      "Training Loss: 0.023901135971148808\n",
      "Test Loss: 0.004695257171988487\n",
      "\n",
      "Epoch: 065\n",
      "Training Loss: 0.02357012778520584\n",
      "Test Loss: 0.004576127044856548\n",
      "\n",
      "Epoch: 066\n",
      "Training Loss: 0.023223879436651867\n",
      "Test Loss: 0.004448046442121267\n",
      "\n",
      "Epoch: 067\n",
      "Training Loss: 0.022895916054646175\n",
      "Test Loss: 0.004325167741626501\n",
      "\n",
      "Epoch: 068\n",
      "Training Loss: 0.022594155743718147\n",
      "Test Loss: 0.004219401627779007\n",
      "\n",
      "Epoch: 069\n",
      "Training Loss: 0.022306935240825016\n",
      "Test Loss: 0.004123345483094454\n",
      "\n",
      "Epoch: 070\n",
      "Training Loss: 0.022036170586943626\n",
      "Test Loss: 0.004031911492347717\n",
      "\n",
      "Epoch: 071\n",
      "Training Loss: 0.02176692585150401\n",
      "Test Loss: 0.0039506531320512295\n",
      "\n",
      "Epoch: 072\n",
      "Training Loss: 0.021537817393740017\n",
      "Test Loss: 0.003892525564879179\n",
      "\n",
      "Epoch: 073\n",
      "Training Loss: 0.02126546824971835\n",
      "Test Loss: 0.0038265525363385677\n",
      "\n",
      "Epoch: 074\n",
      "Training Loss: 0.021112923820813496\n",
      "Test Loss: 0.0037591210566461086\n",
      "\n",
      "Epoch: 075\n",
      "Training Loss: 0.020963452756404877\n",
      "Test Loss: 0.0037048307713121176\n",
      "\n",
      "Epoch: 076\n",
      "Training Loss: 0.0207658801227808\n",
      "Test Loss: 0.003655568463727832\n",
      "\n",
      "Epoch: 077\n",
      "Training Loss: 0.020668661842743557\n",
      "Test Loss: 0.0036059024278074503\n",
      "\n",
      "Epoch: 078\n",
      "Training Loss: 0.020538251226147015\n",
      "Test Loss: 0.003565619233995676\n",
      "\n",
      "Epoch: 079\n",
      "Training Loss: 0.020424955834945042\n",
      "Test Loss: 0.003527306718751788\n",
      "\n",
      "Epoch: 080\n",
      "Training Loss: 0.020201506714026134\n",
      "Test Loss: 0.0034937213640660048\n",
      "\n",
      "Epoch: 081\n",
      "Training Loss: 0.02018314724167188\n",
      "Test Loss: 0.003457465674728155\n",
      "\n",
      "Epoch: 082\n",
      "Training Loss: 0.020050298422574997\n",
      "Test Loss: 0.003427115036174655\n",
      "\n",
      "Epoch: 083\n",
      "Training Loss: 0.019958120460311573\n",
      "Test Loss: 0.00339874648489058\n",
      "\n",
      "Epoch: 084\n",
      "Training Loss: 0.019860730816920597\n",
      "Test Loss: 0.003374911844730377\n",
      "\n",
      "Epoch: 085\n",
      "Training Loss: 0.019754280025760334\n",
      "Test Loss: 0.0033509358763694763\n",
      "\n",
      "Epoch: 086\n",
      "Training Loss: 0.019704310223460197\n",
      "Test Loss: 0.003330327570438385\n",
      "\n",
      "Epoch: 087\n",
      "Training Loss: 0.01960521750152111\n",
      "Test Loss: 0.0033109409268945456\n",
      "\n",
      "Epoch: 088\n",
      "Training Loss: 0.019481393198172253\n",
      "Test Loss: 0.003292917739599943\n",
      "\n",
      "Epoch: 089\n",
      "Training Loss: 0.019452801595131557\n",
      "Test Loss: 0.003275875002145767\n",
      "\n",
      "Epoch: 090\n",
      "Training Loss: 0.01928917256494363\n",
      "Test Loss: 0.003259558929130435\n",
      "\n",
      "Epoch: 091\n",
      "Training Loss: 0.019225303704539936\n",
      "Test Loss: 0.0032432042062282562\n",
      "\n",
      "Epoch: 092\n",
      "Training Loss: 0.01913050500055154\n",
      "Test Loss: 0.0032270634546875954\n",
      "\n",
      "Epoch: 093\n",
      "Training Loss: 0.019066345567504566\n",
      "Test Loss: 0.003210946451872587\n",
      "\n",
      "Epoch: 094\n",
      "Training Loss: 0.018992146477103233\n",
      "Test Loss: 0.0031950182747095823\n",
      "\n",
      "Epoch: 095\n",
      "Training Loss: 0.018761385853091877\n",
      "Test Loss: 0.0031788810156285763\n",
      "\n",
      "Epoch: 096\n",
      "Training Loss: 0.018793446943163872\n",
      "Test Loss: 0.003162616631016135\n",
      "\n",
      "Epoch: 097\n",
      "Training Loss: 0.01873788299659888\n",
      "Test Loss: 0.003146587638184428\n",
      "\n",
      "Epoch: 098\n",
      "Training Loss: 0.01861416424314181\n",
      "Test Loss: 0.0031305986922234297\n",
      "\n",
      "Epoch: 099\n",
      "Training Loss: 0.018519943580031395\n",
      "Test Loss: 0.0031146861147135496\n",
      "\n",
      "Epoch: 100\n",
      "Training Loss: 0.018381886184215546\n",
      "Test Loss: 0.003098865505307913\n",
      "\n",
      "Epoch: 101\n",
      "Training Loss: 0.01838851037124793\n",
      "Test Loss: 0.0030831298790872097\n",
      "\n",
      "Epoch: 102\n",
      "Training Loss: 0.018307732418179512\n",
      "Test Loss: 0.003067723009735346\n",
      "\n",
      "Epoch: 103\n",
      "Training Loss: 0.018139446154236794\n",
      "Test Loss: 0.003052021376788616\n",
      "\n",
      "Epoch: 104\n",
      "Training Loss: 0.018068922062714893\n",
      "Test Loss: 0.0030364040285348892\n",
      "\n",
      "Epoch: 105\n",
      "Training Loss: 0.017935243124763172\n",
      "Test Loss: 0.003021006938070059\n",
      "\n",
      "Epoch: 106\n",
      "Training Loss: 0.017899420112371445\n",
      "Test Loss: 0.0030055521056056023\n",
      "\n",
      "Epoch: 107\n",
      "Training Loss: 0.01781050053735574\n",
      "Test Loss: 0.002990137320011854\n",
      "\n",
      "Epoch: 108\n",
      "Training Loss: 0.01772339145342509\n",
      "Test Loss: 0.002974672010168433\n",
      "\n",
      "Epoch: 109\n",
      "Training Loss: 0.01763674368460973\n",
      "Test Loss: 0.002959362231194973\n",
      "\n",
      "Epoch: 110\n",
      "Training Loss: 0.0174811240285635\n",
      "Test Loss: 0.0029439241625368595\n",
      "\n",
      "Epoch: 111\n",
      "Training Loss: 0.017380721246202786\n",
      "Test Loss: 0.0029285133350640535\n",
      "\n",
      "Epoch: 112\n",
      "Training Loss: 0.01732480836411317\n",
      "Test Loss: 0.002913177479058504\n",
      "\n",
      "Epoch: 113\n",
      "Training Loss: 0.01731290544072787\n",
      "Test Loss: 0.0028978544287383556\n",
      "\n",
      "Epoch: 114\n",
      "Training Loss: 0.017179166277249653\n",
      "Test Loss: 0.0028820696752518415\n",
      "\n",
      "Epoch: 115\n",
      "Training Loss: 0.01700167606274287\n",
      "Test Loss: 0.0028670683968812227\n",
      "\n",
      "Epoch: 116\n",
      "Training Loss: 0.01694733587404092\n",
      "Test Loss: 0.002852071076631546\n",
      "\n",
      "Epoch: 117\n",
      "Training Loss: 0.01685471087694168\n",
      "Test Loss: 0.002836392493918538\n",
      "\n",
      "Epoch: 118\n",
      "Training Loss: 0.016767150722444057\n",
      "Test Loss: 0.0028208859730511904\n",
      "\n",
      "Epoch: 119\n",
      "Training Loss: 0.016689108684659004\n",
      "Test Loss: 0.00280535826459527\n",
      "\n",
      "Epoch: 120\n",
      "Training Loss: 0.016644090724488098\n",
      "Test Loss: 0.002790310187265277\n",
      "\n",
      "Epoch: 121\n",
      "Training Loss: 0.0164375106493632\n",
      "Test Loss: 0.002774574561044574\n",
      "\n",
      "Epoch: 122\n",
      "Training Loss: 0.01642768271267414\n",
      "Test Loss: 0.002759207971394062\n",
      "\n",
      "Epoch: 123\n",
      "Training Loss: 0.01637199365844329\n",
      "Test Loss: 0.002743770368397236\n",
      "\n",
      "Epoch: 124\n",
      "Training Loss: 0.016276858747005463\n",
      "Test Loss: 0.0027284130919724703\n",
      "\n",
      "Epoch: 125\n",
      "Training Loss: 0.01610372867435217\n",
      "Test Loss: 0.002712861867621541\n",
      "\n",
      "Epoch: 126\n",
      "Training Loss: 0.016096191480755806\n",
      "Test Loss: 0.002697551855817437\n",
      "\n",
      "Epoch: 127\n",
      "Training Loss: 0.015989994630217552\n",
      "Test Loss: 0.0026820660568773746\n",
      "\n",
      "Epoch: 128\n",
      "Training Loss: 0.0159039714684089\n",
      "Test Loss: 0.0026665322948247194\n",
      "\n",
      "Epoch: 129\n",
      "Training Loss: 0.015707232989370823\n",
      "Test Loss: 0.0026509861927479506\n",
      "\n",
      "Epoch: 130\n",
      "Training Loss: 0.015694054154058296\n",
      "Test Loss: 0.002635733690112829\n",
      "\n",
      "Epoch: 131\n",
      "Training Loss: 0.015588400575021902\n",
      "Test Loss: 0.0026239571161568165\n",
      "\n",
      "Epoch: 132\n",
      "Training Loss: 0.01550002240886291\n",
      "Test Loss: 0.002604779787361622\n",
      "\n",
      "Epoch: 133\n",
      "Training Loss: 0.015369457503159841\n",
      "Test Loss: 0.002591657917946577\n",
      "\n",
      "Epoch: 134\n",
      "Training Loss: 0.015324940904974937\n",
      "Test Loss: 0.002575488528236747\n",
      "\n",
      "Epoch: 135\n",
      "Training Loss: 0.015216420715053877\n",
      "Test Loss: 0.002558651380240917\n",
      "\n",
      "Epoch: 136\n",
      "Training Loss: 0.01517501100897789\n",
      "Test Loss: 0.002543058944866061\n",
      "\n",
      "Epoch: 137\n",
      "Training Loss: 0.015075286229451498\n",
      "Test Loss: 0.002527575707063079\n",
      "\n",
      "Epoch: 138\n",
      "Training Loss: 0.015004723022381464\n",
      "Test Loss: 0.0025121141225099564\n",
      "\n",
      "Epoch: 139\n",
      "Training Loss: 0.01487906618664662\n",
      "Test Loss: 0.0024965889751911163\n",
      "\n",
      "Epoch: 140\n",
      "Training Loss: 0.01481702458113432\n",
      "Test Loss: 0.0024808256421238184\n",
      "\n",
      "Epoch: 141\n",
      "Training Loss: 0.014692756036917368\n",
      "Test Loss: 0.0024650259874761105\n",
      "\n",
      "Epoch: 142\n",
      "Training Loss: 0.014585060688356558\n",
      "Test Loss: 0.002449548337608576\n",
      "\n",
      "Epoch: 143\n",
      "Training Loss: 0.014488591812551022\n",
      "Test Loss: 0.0024338483344763517\n",
      "\n",
      "Epoch: 144\n",
      "Training Loss: 0.01441236212849617\n",
      "Test Loss: 0.0024185983929783106\n",
      "\n",
      "Epoch: 145\n",
      "Training Loss: 0.014256374910473824\n",
      "Test Loss: 0.002403074875473976\n",
      "\n",
      "Epoch: 146\n",
      "Training Loss: 0.014187802250186602\n",
      "Test Loss: 0.0023872621823102236\n",
      "\n",
      "Epoch: 147\n",
      "Training Loss: 0.014124011931320032\n",
      "Test Loss: 0.0023718029260635376\n",
      "\n",
      "Epoch: 148\n",
      "Training Loss: 0.014056400706370672\n",
      "Test Loss: 0.00235624797642231\n",
      "\n",
      "Epoch: 149\n",
      "Training Loss: 0.013950276498993238\n",
      "Test Loss: 0.002340887440368533\n",
      "\n",
      "Epoch: 150\n",
      "Training Loss: 0.013831139852603277\n",
      "Test Loss: 0.00232549081556499\n",
      "\n",
      "Epoch: 151\n",
      "Training Loss: 0.013745245213309923\n",
      "Test Loss: 0.0023097528610378504\n",
      "\n",
      "Epoch: 152\n",
      "Training Loss: 0.013684551231563091\n",
      "Test Loss: 0.0022943562362343073\n",
      "\n",
      "Epoch: 153\n",
      "Training Loss: 0.013576415367424488\n",
      "Test Loss: 0.0022788874339312315\n",
      "\n",
      "Epoch: 154\n",
      "Training Loss: 0.01353331096470356\n",
      "Test Loss: 0.002263424452394247\n",
      "\n",
      "Epoch: 155\n",
      "Training Loss: 0.013436437584459782\n",
      "Test Loss: 0.00224786764010787\n",
      "\n",
      "Epoch: 156\n",
      "Training Loss: 0.013299458349744478\n",
      "Test Loss: 0.002232540166005492\n",
      "\n",
      "Epoch: 157\n",
      "Training Loss: 0.013215905365844568\n",
      "Test Loss: 0.0022169973235577345\n",
      "\n",
      "Epoch: 158\n",
      "Training Loss: 0.013123278506100178\n",
      "Test Loss: 0.002201905706897378\n",
      "\n",
      "Epoch: 159\n",
      "Training Loss: 0.013031324992577234\n",
      "Test Loss: 0.0021861265413463116\n",
      "\n",
      "Epoch: 160\n",
      "Training Loss: 0.01294898179670175\n",
      "Test Loss: 0.0021707641426473856\n",
      "\n",
      "Epoch: 161\n",
      "Training Loss: 0.012853285297751427\n",
      "Test Loss: 0.002155388006940484\n",
      "\n",
      "Epoch: 162\n",
      "Training Loss: 0.012761432367066542\n",
      "Test Loss: 0.002139908028766513\n",
      "\n",
      "Epoch: 163\n",
      "Training Loss: 0.01263949895898501\n",
      "Test Loss: 0.0021250704303383827\n",
      "\n",
      "Epoch: 164\n",
      "Training Loss: 0.012560371619959673\n",
      "Test Loss: 0.002112603047862649\n",
      "\n",
      "Epoch: 165\n",
      "Training Loss: 0.012498013054331144\n",
      "Test Loss: 0.002097521675750613\n",
      "\n",
      "Epoch: 166\n",
      "Training Loss: 0.012404050367573896\n",
      "Test Loss: 0.0020813157316297293\n",
      "\n",
      "Epoch: 167\n",
      "Training Loss: 0.012313618324697018\n",
      "Test Loss: 0.0020635740365833044\n",
      "\n",
      "Epoch: 168\n",
      "Training Loss: 0.012197749068339666\n",
      "Test Loss: 0.002048567170277238\n",
      "\n",
      "Epoch: 169\n",
      "Training Loss: 0.012076130757729212\n",
      "Test Loss: 0.002033728640526533\n",
      "\n",
      "Epoch: 170\n",
      "Training Loss: 0.012039967191716036\n",
      "Test Loss: 0.0020177417900413275\n",
      "\n",
      "Epoch: 171\n",
      "Training Loss: 0.01188695648064216\n",
      "Test Loss: 0.0020020385272800922\n",
      "\n",
      "Epoch: 172\n",
      "Training Loss: 0.011796025248865286\n",
      "Test Loss: 0.00198724796064198\n",
      "\n",
      "Epoch: 173\n",
      "Training Loss: 0.01175505481660366\n",
      "Test Loss: 0.0019718955736607313\n",
      "\n",
      "Epoch: 174\n",
      "Training Loss: 0.011644859177370867\n",
      "Test Loss: 0.0019562116358429193\n",
      "\n",
      "Epoch: 175\n",
      "Training Loss: 0.011573264996210733\n",
      "Test Loss: 0.0019409714732319117\n",
      "\n",
      "Epoch: 176\n",
      "Training Loss: 0.011464010924100876\n",
      "Test Loss: 0.001925874501466751\n",
      "\n",
      "Epoch: 177\n",
      "Training Loss: 0.011370937339961529\n",
      "Test Loss: 0.0019106133840978146\n",
      "\n",
      "Epoch: 178\n",
      "Training Loss: 0.011335466988384724\n",
      "Test Loss: 0.001895649591460824\n",
      "\n",
      "Epoch: 179\n",
      "Training Loss: 0.01120090422530969\n",
      "Test Loss: 0.0018803393468260765\n",
      "\n",
      "Epoch: 180\n",
      "Training Loss: 0.011108047639330229\n",
      "Test Loss: 0.0018652150174602866\n",
      "\n",
      "Epoch: 181\n",
      "Training Loss: 0.010992333913842836\n",
      "Test Loss: 0.0018501407466828823\n",
      "\n",
      "Epoch: 182\n",
      "Training Loss: 0.010946597593526045\n",
      "Test Loss: 0.0018351073376834393\n",
      "\n",
      "Epoch: 183\n",
      "Training Loss: 0.01086787786334753\n",
      "Test Loss: 0.001820322242565453\n",
      "\n",
      "Epoch: 184\n",
      "Training Loss: 0.010762177718182405\n",
      "Test Loss: 0.0018050521612167358\n",
      "\n",
      "Epoch: 185\n",
      "Training Loss: 0.010586687984565893\n",
      "Test Loss: 0.0017899961676448584\n",
      "\n",
      "Epoch: 186\n",
      "Training Loss: 0.010584152614076933\n",
      "Test Loss: 0.001775024808011949\n",
      "\n",
      "Epoch: 187\n",
      "Training Loss: 0.010496942015985647\n",
      "Test Loss: 0.0017604220192879438\n",
      "\n",
      "Epoch: 188\n",
      "Training Loss: 0.010373170177141825\n",
      "Test Loss: 0.0017461910611018538\n",
      "\n",
      "Epoch: 189\n",
      "Training Loss: 0.010326992720365524\n",
      "Test Loss: 0.001734319026581943\n",
      "\n",
      "Epoch: 190\n",
      "Training Loss: 0.010222050050894419\n",
      "Test Loss: 0.0017160255229100585\n",
      "\n",
      "Epoch: 191\n",
      "Training Loss: 0.0101364611958464\n",
      "Test Loss: 0.0017012340249493718\n",
      "\n",
      "Epoch: 192\n",
      "Training Loss: 0.010043925295273462\n",
      "Test Loss: 0.0016870182007551193\n",
      "\n",
      "Epoch: 193\n",
      "Training Loss: 0.009959664816657702\n",
      "Test Loss: 0.001671274658292532\n",
      "\n",
      "Epoch: 194\n",
      "Training Loss: 0.009865950172146162\n",
      "Test Loss: 0.0016565904952585697\n",
      "\n",
      "Epoch: 195\n",
      "Training Loss: 0.009818962464729944\n",
      "Test Loss: 0.001641503069549799\n",
      "\n",
      "Epoch: 196\n",
      "Training Loss: 0.009681873644391695\n",
      "Test Loss: 0.001627066871151328\n",
      "\n",
      "Epoch: 197\n",
      "Training Loss: 0.00958566584934791\n",
      "Test Loss: 0.0016119912033900619\n",
      "\n",
      "Epoch: 198\n",
      "Training Loss: 0.009518683577577272\n",
      "Test Loss: 0.0015973271802067757\n",
      "\n",
      "Epoch: 199\n",
      "Training Loss: 0.009393063684304556\n",
      "Test Loss: 0.0015822888817638159\n",
      "\n",
      "Epoch: 200\n",
      "Training Loss: 0.009304196573793888\n",
      "Test Loss: 0.001567549305036664\n",
      "\n",
      "Epoch: 201\n",
      "Training Loss: 0.00924680009484291\n",
      "Test Loss: 0.0015527948271483183\n",
      "\n",
      "Epoch: 202\n",
      "Training Loss: 0.009168016413847605\n",
      "Test Loss: 0.0015380484983325005\n",
      "\n",
      "Epoch: 203\n",
      "Training Loss: 0.009073654189705849\n",
      "Test Loss: 0.001523693441413343\n",
      "\n",
      "Epoch: 204\n",
      "Training Loss: 0.008965290151536465\n",
      "Test Loss: 0.0015091104432940483\n",
      "\n",
      "Epoch: 205\n",
      "Training Loss: 0.008869739870230356\n",
      "Test Loss: 0.0014950563199818134\n",
      "\n",
      "Epoch: 206\n",
      "Training Loss: 0.008822928803662458\n",
      "Test Loss: 0.0014818330528214574\n",
      "\n",
      "Epoch: 207\n",
      "Training Loss: 0.008732327570517858\n",
      "Test Loss: 0.0014670679811388254\n",
      "\n",
      "Epoch: 208\n",
      "Training Loss: 0.008605303553243479\n",
      "Test Loss: 0.001452570897527039\n",
      "\n",
      "Epoch: 209\n",
      "Training Loss: 0.008566499687731266\n",
      "Test Loss: 0.001437908154912293\n",
      "\n",
      "Epoch: 210\n",
      "Training Loss: 0.008483720012009144\n",
      "Test Loss: 0.0014240546151995659\n",
      "\n",
      "Epoch: 211\n",
      "Training Loss: 0.008368158557762703\n",
      "Test Loss: 0.001409389078617096\n",
      "\n",
      "Epoch: 212\n",
      "Training Loss: 0.008271412923932076\n",
      "Test Loss: 0.00139559933450073\n",
      "\n",
      "Epoch: 213\n",
      "Training Loss: 0.00820601498708129\n",
      "Test Loss: 0.0013816393911838531\n",
      "\n",
      "Epoch: 214\n",
      "Training Loss: 0.00814160363127788\n",
      "Test Loss: 0.0013675488298758864\n",
      "\n",
      "Epoch: 215\n",
      "Training Loss: 0.008032844557116428\n",
      "Test Loss: 0.0013585794949904084\n",
      "\n",
      "Epoch: 216\n",
      "Training Loss: 0.00796801426137487\n",
      "Test Loss: 0.0013407837832346559\n",
      "\n",
      "Epoch: 217\n",
      "Training Loss: 0.007877982687205076\n",
      "Test Loss: 0.0013299991842359304\n",
      "\n",
      "Epoch: 218\n",
      "Training Loss: 0.007784590435524781\n",
      "Test Loss: 0.0013143899850547314\n",
      "\n",
      "Epoch: 219\n",
      "Training Loss: 0.0077211543296774226\n",
      "Test Loss: 0.0012985927751287818\n",
      "\n",
      "Epoch: 220\n",
      "Training Loss: 0.007638125214725733\n",
      "Test Loss: 0.0012843783479183912\n",
      "\n",
      "Epoch: 221\n",
      "Training Loss: 0.007544558650503556\n",
      "Test Loss: 0.0012709852308034897\n",
      "\n",
      "Epoch: 222\n",
      "Training Loss: 0.007460849359631538\n",
      "Test Loss: 0.0012567489175125957\n",
      "\n",
      "Epoch: 223\n",
      "Training Loss: 0.007405439391732216\n",
      "Test Loss: 0.0012428438058122993\n",
      "\n",
      "Epoch: 224\n",
      "Training Loss: 0.007320414607723554\n",
      "Test Loss: 0.0012303225230425596\n",
      "\n",
      "Epoch: 225\n",
      "Training Loss: 0.0072168234425286455\n",
      "Test Loss: 0.0012166041415184736\n",
      "\n",
      "Epoch: 226\n",
      "Training Loss: 0.007129176364590724\n",
      "Test Loss: 0.0012025629403069615\n",
      "\n",
      "Epoch: 227\n",
      "Training Loss: 0.007079578780879577\n",
      "Test Loss: 0.0011892126640304923\n",
      "\n",
      "Epoch: 228\n",
      "Training Loss: 0.006999649417897065\n",
      "Test Loss: 0.0011760356137529016\n",
      "\n",
      "Epoch: 229\n",
      "Training Loss: 0.0069410062084595365\n",
      "Test Loss: 0.001162637141533196\n",
      "\n",
      "Epoch: 230\n",
      "Training Loss: 0.00683447594443957\n",
      "Test Loss: 0.0011494053760543466\n",
      "\n",
      "Epoch: 231\n",
      "Training Loss: 0.006723450341572364\n",
      "Test Loss: 0.001136349281296134\n",
      "\n",
      "Epoch: 232\n",
      "Training Loss: 0.006678475687901179\n",
      "Test Loss: 0.0011231328826397657\n",
      "\n",
      "Epoch: 233\n",
      "Training Loss: 0.0065522948279976845\n",
      "Test Loss: 0.0011099142720922828\n",
      "\n",
      "Epoch: 234\n",
      "Training Loss: 0.006490948610007763\n",
      "Test Loss: 0.0010969963623210788\n",
      "\n",
      "Epoch: 235\n",
      "Training Loss: 0.006445747489730517\n",
      "Test Loss: 0.0010840232716873288\n",
      "\n",
      "Epoch: 236\n",
      "Training Loss: 0.006365227357794841\n",
      "Test Loss: 0.001071035279892385\n",
      "\n",
      "Epoch: 237\n",
      "Training Loss: 0.006281593348830938\n",
      "Test Loss: 0.0010583060793578625\n",
      "\n",
      "Epoch: 238\n",
      "Training Loss: 0.0062178887116412325\n",
      "Test Loss: 0.001045689801685512\n",
      "\n",
      "Epoch: 239\n",
      "Training Loss: 0.006140196385482947\n",
      "Test Loss: 0.0010337266139686108\n",
      "\n",
      "Epoch: 240\n",
      "Training Loss: 0.006067816788951556\n",
      "Test Loss: 0.0010228834580630064\n",
      "\n",
      "Epoch: 241\n",
      "Training Loss: 0.005987384666999181\n",
      "Test Loss: 0.001013450208120048\n",
      "\n",
      "Epoch: 242\n",
      "Training Loss: 0.005911606674393018\n",
      "Test Loss: 0.0009966174839064479\n",
      "\n",
      "Epoch: 243\n",
      "Training Loss: 0.005821705951044957\n",
      "Test Loss: 0.0009831673232838511\n",
      "\n",
      "Epoch: 244\n",
      "Training Loss: 0.0057483273558318615\n",
      "Test Loss: 0.0009709416190162301\n",
      "\n",
      "Epoch: 245\n",
      "Training Loss: 0.00568390591070056\n",
      "Test Loss: 0.000958434131462127\n",
      "\n",
      "Epoch: 246\n",
      "Training Loss: 0.005608320546646913\n",
      "Test Loss: 0.0009463986498303711\n",
      "\n",
      "Epoch: 247\n",
      "Training Loss: 0.005527037351081769\n",
      "Test Loss: 0.0009340665419586003\n",
      "\n",
      "Epoch: 248\n",
      "Training Loss: 0.0054731980587045355\n",
      "Test Loss: 0.0009220308857038617\n",
      "\n",
      "Epoch: 249\n",
      "Training Loss: 0.005411492350200812\n",
      "Test Loss: 0.0009100841707549989\n",
      "\n",
      "Epoch: 250\n",
      "Training Loss: 0.0053147537012894945\n",
      "Test Loss: 0.0008981848950497806\n",
      "\n",
      "Epoch: 251\n",
      "Training Loss: 0.005261649377644062\n",
      "Test Loss: 0.0008863418479450047\n",
      "\n",
      "Epoch: 252\n",
      "Training Loss: 0.005193166279544433\n",
      "Test Loss: 0.0008750702836550772\n",
      "\n",
      "Epoch: 253\n",
      "Training Loss: 0.005128070867309968\n",
      "Test Loss: 0.0008630925440229475\n",
      "\n",
      "Epoch: 254\n",
      "Training Loss: 0.00505240090812246\n",
      "Test Loss: 0.0008515228983014822\n",
      "\n",
      "Epoch: 255\n",
      "Training Loss: 0.00496541786318024\n",
      "Test Loss: 0.0008397916681133211\n",
      "\n",
      "Epoch: 256\n",
      "Training Loss: 0.004905457763622205\n",
      "Test Loss: 0.0008283971692435443\n",
      "\n",
      "Epoch: 257\n",
      "Training Loss: 0.004839729517698288\n",
      "Test Loss: 0.000817103951703757\n",
      "\n",
      "Epoch: 258\n",
      "Training Loss: 0.00477178068831563\n",
      "Test Loss: 0.0008060695836320519\n",
      "\n",
      "Epoch: 259\n",
      "Training Loss: 0.004711862963934739\n",
      "Test Loss: 0.0007951603620313108\n",
      "\n",
      "Epoch: 260\n",
      "Training Loss: 0.004645855166018009\n",
      "Test Loss: 0.0007841848419047892\n",
      "\n",
      "Epoch: 261\n",
      "Training Loss: 0.004588463964561622\n",
      "Test Loss: 0.0007732927333563566\n",
      "\n",
      "Epoch: 262\n",
      "Training Loss: 0.004522805257389943\n",
      "Test Loss: 0.0007624872378073633\n",
      "\n",
      "Epoch: 263\n",
      "Training Loss: 0.004448240467657645\n",
      "Test Loss: 0.0007519626524299383\n",
      "\n",
      "Epoch: 264\n",
      "Training Loss: 0.004378587317963441\n",
      "Test Loss: 0.0007410591351799667\n",
      "\n",
      "Epoch: 265\n",
      "Training Loss: 0.0043125757947564125\n",
      "Test Loss: 0.0007304360624402761\n",
      "\n",
      "Epoch: 266\n",
      "Training Loss: 0.004251244477927685\n",
      "Test Loss: 0.0007199588580988348\n",
      "\n",
      "Epoch: 267\n",
      "Training Loss: 0.004203329716498653\n",
      "Test Loss: 0.0007095267064869404\n",
      "\n",
      "Epoch: 268\n",
      "Training Loss: 0.004122796390826504\n",
      "Test Loss: 0.000699162483215332\n",
      "\n",
      "Epoch: 269\n",
      "Training Loss: 0.004073334081719319\n",
      "Test Loss: 0.000688891508616507\n",
      "\n",
      "Epoch: 270\n",
      "Training Loss: 0.004011924068133037\n",
      "Test Loss: 0.0006786659942008555\n",
      "\n",
      "Epoch: 271\n",
      "Training Loss: 0.003952403319999576\n",
      "Test Loss: 0.0006686730193905532\n",
      "\n",
      "Epoch: 272\n",
      "Training Loss: 0.003883164841681719\n",
      "Test Loss: 0.0006585042574442923\n",
      "\n",
      "Epoch: 273\n",
      "Training Loss: 0.0038313777185976505\n",
      "Test Loss: 0.0006488625076599419\n",
      "\n",
      "Epoch: 274\n",
      "Training Loss: 0.0037617264315485954\n",
      "Test Loss: 0.0006387542234733701\n",
      "\n",
      "Epoch: 275\n",
      "Training Loss: 0.0037163589149713516\n",
      "Test Loss: 0.0006291134632192552\n",
      "\n",
      "Epoch: 276\n",
      "Training Loss: 0.0036435741931200027\n",
      "Test Loss: 0.0006192073924466968\n",
      "\n",
      "Epoch: 277\n",
      "Training Loss: 0.003600148561721047\n",
      "Test Loss: 0.0006103303749114275\n",
      "\n",
      "Epoch: 278\n",
      "Training Loss: 0.003521999421839913\n",
      "Test Loss: 0.0005999992717988789\n",
      "\n",
      "Epoch: 279\n",
      "Training Loss: 0.003481929268067082\n",
      "Test Loss: 0.0005907522863708436\n",
      "\n",
      "Epoch: 280\n",
      "Training Loss: 0.0034327621882160506\n",
      "Test Loss: 0.0005815241602249444\n",
      "\n",
      "Epoch: 281\n",
      "Training Loss: 0.0033787848272671304\n",
      "Test Loss: 0.0005737481405958533\n",
      "\n",
      "Epoch: 282\n",
      "Training Loss: 0.0033234748213241496\n",
      "Test Loss: 0.0005650103557854891\n",
      "\n",
      "Epoch: 283\n",
      "Training Loss: 0.0032624572049826384\n",
      "Test Loss: 0.0005544025916606188\n",
      "\n",
      "Epoch: 284\n",
      "Training Loss: 0.003212910300741593\n",
      "Test Loss: 0.0005458451341837645\n",
      "\n",
      "Epoch: 285\n",
      "Training Loss: 0.0031259602401405573\n",
      "Test Loss: 0.0005361466901376843\n",
      "\n",
      "Epoch: 286\n",
      "Training Loss: 0.003111590708916386\n",
      "Test Loss: 0.0005272178095765412\n",
      "\n",
      "Epoch: 287\n",
      "Training Loss: 0.0030478322878479958\n",
      "Test Loss: 0.0005185522022657096\n",
      "\n",
      "Epoch: 288\n",
      "Training Loss: 0.003006375472371777\n",
      "Test Loss: 0.0005099271656945348\n",
      "\n",
      "Epoch: 289\n",
      "Training Loss: 0.0029487095307558775\n",
      "Test Loss: 0.0005014148773625493\n",
      "\n",
      "Epoch: 290\n",
      "Training Loss: 0.0029033205937594175\n",
      "Test Loss: 0.0004929160932078958\n",
      "\n",
      "Epoch: 291\n",
      "Training Loss: 0.0028504147194325924\n",
      "Test Loss: 0.0004845579096581787\n",
      "\n",
      "Epoch: 292\n",
      "Training Loss: 0.002785820203522841\n",
      "Test Loss: 0.00047612478374503553\n",
      "\n",
      "Epoch: 293\n",
      "Training Loss: 0.00275157717987895\n",
      "Test Loss: 0.00046789454063400626\n",
      "\n",
      "Epoch: 294\n",
      "Training Loss: 0.0027078031562268734\n",
      "Test Loss: 0.00046000172733329237\n",
      "\n",
      "Epoch: 295\n",
      "Training Loss: 0.002656712274377545\n",
      "Test Loss: 0.0004521092341747135\n",
      "\n",
      "Epoch: 296\n",
      "Training Loss: 0.002609467521930734\n",
      "Test Loss: 0.0004443886282388121\n",
      "\n",
      "Epoch: 297\n",
      "Training Loss: 0.002558100347717603\n",
      "Test Loss: 0.00043613434536382556\n",
      "\n",
      "Epoch: 298\n",
      "Training Loss: 0.0025116735293219485\n",
      "Test Loss: 0.0004283643211238086\n",
      "\n",
      "Epoch: 299\n",
      "Training Loss: 0.002468409016728401\n",
      "Test Loss: 0.00042114820098504424\n",
      "\n",
      "Epoch: 300\n",
      "Training Loss: 0.002425223356112838\n",
      "Test Loss: 0.00041337255970574915\n",
      "\n",
      "Epoch: 301\n",
      "Training Loss: 0.002374932635575533\n",
      "Test Loss: 0.0004058338236063719\n",
      "\n",
      "Epoch: 302\n",
      "Training Loss: 0.002325371994326512\n",
      "Test Loss: 0.00039832890615798533\n",
      "\n",
      "Epoch: 303\n",
      "Training Loss: 0.002293479163199663\n",
      "Test Loss: 0.00039113915408961475\n",
      "\n",
      "Epoch: 304\n",
      "Training Loss: 0.0022508291682849326\n",
      "Test Loss: 0.0003837831609416753\n",
      "\n",
      "Epoch: 305\n",
      "Training Loss: 0.0022008673598368964\n",
      "Test Loss: 0.00037681363755837083\n",
      "\n",
      "Epoch: 306\n",
      "Training Loss: 0.0021632774733006954\n",
      "Test Loss: 0.00036964649916626513\n",
      "\n",
      "Epoch: 307\n",
      "Training Loss: 0.0021186995630462966\n",
      "Test Loss: 0.00036271914723329246\n",
      "\n",
      "Epoch: 308\n",
      "Training Loss: 0.002086672388638059\n",
      "Test Loss: 0.0003741223190445453\n",
      "\n",
      "Epoch: 309\n",
      "Training Loss: 0.002083154550443093\n",
      "Test Loss: 0.0003577766183298081\n",
      "\n",
      "Epoch: 310\n",
      "Training Loss: 0.0020171931634346643\n",
      "Test Loss: 0.000368770444765687\n",
      "\n",
      "Epoch: 311\n",
      "Training Loss: 0.001982717076316476\n",
      "Test Loss: 0.0003479683364275843\n",
      "\n",
      "Epoch: 312\n",
      "Training Loss: 0.0019348923427363236\n",
      "Test Loss: 0.00033695512684062123\n",
      "\n",
      "Epoch: 313\n",
      "Training Loss: 0.0018899758191158373\n",
      "Test Loss: 0.0003404403105378151\n",
      "\n",
      "Epoch: 314\n",
      "Training Loss: 0.0018579014965022604\n",
      "Test Loss: 0.00031979664345271885\n",
      "\n",
      "Epoch: 315\n",
      "Training Loss: 0.001817438091772298\n",
      "Test Loss: 0.00031310744816437364\n",
      "\n",
      "Epoch: 316\n",
      "Training Loss: 0.001777716912329197\n",
      "Test Loss: 0.0003117006563115865\n",
      "\n",
      "Epoch: 317\n",
      "Training Loss: 0.001739619066938758\n",
      "Test Loss: 0.00029955687932670116\n",
      "\n",
      "Epoch: 318\n",
      "Training Loss: 0.0017091694753617048\n",
      "Test Loss: 0.0002931456547230482\n",
      "\n",
      "Epoch: 319\n",
      "Training Loss: 0.0016705637487272422\n",
      "Test Loss: 0.0002897144004236907\n",
      "\n",
      "Epoch: 320\n",
      "Training Loss: 0.0016322034255911906\n",
      "Test Loss: 0.0002810622681863606\n",
      "\n",
      "Epoch: 321\n",
      "Training Loss: 0.001604137282508115\n",
      "Test Loss: 0.00027500803116708994\n",
      "\n",
      "Epoch: 322\n",
      "Training Loss: 0.0015701570858558018\n",
      "Test Loss: 0.0002702913188841194\n",
      "\n",
      "Epoch: 323\n",
      "Training Loss: 0.0015295216192801793\n",
      "Test Loss: 0.0002636534336488694\n",
      "\n",
      "Epoch: 324\n",
      "Training Loss: 0.0015041264705359936\n",
      "Test Loss: 0.0002580130530986935\n",
      "\n",
      "Epoch: 325\n",
      "Training Loss: 0.0014722953007246058\n",
      "Test Loss: 0.0002529565244913101\n",
      "\n",
      "Epoch: 326\n",
      "Training Loss: 0.001430707867257297\n",
      "Test Loss: 0.00024714882601983845\n",
      "\n",
      "Epoch: 327\n",
      "Training Loss: 0.0014032149532188971\n",
      "Test Loss: 0.00024179565662052482\n",
      "\n",
      "Epoch: 328\n",
      "Training Loss: 0.0013743920329337318\n",
      "Test Loss: 0.00023658938880544156\n",
      "\n",
      "Epoch: 329\n",
      "Training Loss: 0.0013485477538779378\n",
      "Test Loss: 0.00023134560615289956\n",
      "\n",
      "Epoch: 330\n",
      "Training Loss: 0.0013141804762805502\n",
      "Test Loss: 0.00022625052952207625\n",
      "\n",
      "Epoch: 331\n",
      "Training Loss: 0.0012882363904888432\n",
      "Test Loss: 0.0002212577819591388\n",
      "\n",
      "Epoch: 332\n",
      "Training Loss: 0.0012529796222224832\n",
      "Test Loss: 0.00021637781173922122\n",
      "\n",
      "Epoch: 333\n",
      "Training Loss: 0.0012200159253552556\n",
      "Test Loss: 0.0002115675451932475\n",
      "\n",
      "Epoch: 334\n",
      "Training Loss: 0.0012048052158206701\n",
      "Test Loss: 0.00020687714277300984\n",
      "\n",
      "Epoch: 335\n",
      "Training Loss: 0.0011793703694517414\n",
      "Test Loss: 0.00020228374341968447\n",
      "\n",
      "Epoch: 336\n",
      "Training Loss: 0.0011466307332739234\n",
      "Test Loss: 0.00019772988162003458\n",
      "\n",
      "Epoch: 337\n",
      "Training Loss: 0.0011223720309014122\n",
      "Test Loss: 0.00019323310698382556\n",
      "\n",
      "Epoch: 338\n",
      "Training Loss: 0.0011007982539013028\n",
      "Test Loss: 0.0001888262340798974\n",
      "\n",
      "Epoch: 339\n",
      "Training Loss: 0.0010703453056824703\n",
      "Test Loss: 0.00018449229537509382\n",
      "\n",
      "Epoch: 340\n",
      "Training Loss: 0.0010479665749395888\n",
      "Test Loss: 0.00018022894801106304\n",
      "\n",
      "Epoch: 341\n",
      "Training Loss: 0.0010188475910884638\n",
      "Test Loss: 0.00017603790911380202\n",
      "\n",
      "Epoch: 342\n",
      "Training Loss: 0.0009952228477535148\n",
      "Test Loss: 0.00017195293912664056\n",
      "\n",
      "Epoch: 343\n",
      "Training Loss: 0.0009713265656804045\n",
      "Test Loss: 0.0001679057313594967\n",
      "\n",
      "Epoch: 344\n",
      "Training Loss: 0.0009531071021532019\n",
      "Test Loss: 0.0001639430847717449\n",
      "\n",
      "Epoch: 345\n",
      "Training Loss: 0.0009297280145498613\n",
      "Test Loss: 0.0001600433752173558\n",
      "\n",
      "Epoch: 346\n",
      "Training Loss: 0.0009072706064519783\n",
      "Test Loss: 0.00015621856437064707\n",
      "\n",
      "Epoch: 347\n",
      "Training Loss: 0.0008818586162912349\n",
      "Test Loss: 0.00015246111433953047\n",
      "\n",
      "Epoch: 348\n",
      "Training Loss: 0.0008645885003109773\n",
      "Test Loss: 0.0001487616973463446\n",
      "\n",
      "Epoch: 349\n",
      "Training Loss: 0.0008396445385490855\n",
      "Test Loss: 0.00014515049406327307\n",
      "\n",
      "Epoch: 350\n",
      "Training Loss: 0.0008199552733761569\n",
      "Test Loss: 0.00014160976570565253\n",
      "\n",
      "Epoch: 351\n",
      "Training Loss: 0.0008034774800762534\n",
      "Test Loss: 0.0001381097245030105\n",
      "\n",
      "Epoch: 352\n",
      "Training Loss: 0.0007797209934021035\n",
      "Test Loss: 0.00013482804934028536\n",
      "\n",
      "Epoch: 353\n",
      "Training Loss: 0.000763306743465364\n",
      "Test Loss: 0.00013133275206200778\n",
      "\n",
      "Epoch: 354\n",
      "Training Loss: 0.0007438061681265632\n",
      "Test Loss: 0.00012803413846995682\n",
      "\n",
      "Epoch: 355\n",
      "Training Loss: 0.0007275134751883646\n",
      "Test Loss: 0.00012480700388550758\n",
      "\n",
      "Epoch: 356\n",
      "Training Loss: 0.0007068559255761405\n",
      "Test Loss: 0.00012164044164819643\n",
      "\n",
      "Epoch: 357\n",
      "Training Loss: 0.0006836588145233691\n",
      "Test Loss: 0.00011853897012770176\n",
      "\n",
      "Epoch: 358\n",
      "Training Loss: 0.0006695058352003495\n",
      "Test Loss: 0.00011551762872841209\n",
      "\n",
      "Epoch: 359\n",
      "Training Loss: 0.0006529428452874223\n",
      "Test Loss: 0.00011253420234424993\n",
      "\n",
      "Epoch: 360\n",
      "Training Loss: 0.0006344834109768271\n",
      "Test Loss: 0.00010962557280436158\n",
      "\n",
      "Epoch: 361\n",
      "Training Loss: 0.0006180004760002097\n",
      "Test Loss: 0.00010678374383132905\n",
      "\n",
      "Epoch: 362\n",
      "Training Loss: 0.0005977665229390065\n",
      "Test Loss: 0.00010398504673503339\n",
      "\n",
      "Epoch: 363\n",
      "Training Loss: 0.0005872451971905927\n",
      "Test Loss: 0.00010125017433892936\n",
      "\n",
      "Epoch: 364\n",
      "Training Loss: 0.0005697349358039597\n",
      "Test Loss: 9.8556381999515e-05\n",
      "\n",
      "Epoch: 365\n",
      "Training Loss: 0.0005544350909379622\n",
      "Test Loss: 9.600607154425234e-05\n",
      "\n",
      "Epoch: 366\n",
      "Training Loss: 0.000541773314277331\n",
      "Test Loss: 9.337704977951944e-05\n",
      "\n",
      "Epoch: 367\n",
      "Training Loss: 0.0005273069449079534\n",
      "Test Loss: 9.086164936888963e-05\n",
      "\n",
      "Epoch: 368\n",
      "Training Loss: 0.0005130530917085707\n",
      "Test Loss: 8.842524403007701e-05\n",
      "\n",
      "Epoch: 369\n",
      "Training Loss: 0.0004980653951254984\n",
      "Test Loss: 8.602550224168226e-05\n",
      "\n",
      "Epoch: 370\n",
      "Training Loss: 0.0004850573604926467\n",
      "Test Loss: 8.36853650980629e-05\n",
      "\n",
      "Epoch: 371\n",
      "Training Loss: 0.00047236642179389793\n",
      "Test Loss: 8.140160207403824e-05\n",
      "\n",
      "Epoch: 372\n",
      "Training Loss: 0.0004586088277089099\n",
      "Test Loss: 7.917426410131156e-05\n",
      "\n",
      "Epoch: 373\n",
      "Training Loss: 0.0004455741533699135\n",
      "Test Loss: 7.700142305111513e-05\n",
      "\n",
      "Epoch: 374\n",
      "Training Loss: 0.0004329662284969042\n",
      "Test Loss: 7.498865306843072e-05\n",
      "\n",
      "Epoch: 375\n",
      "Training Loss: 0.0004193877102807164\n",
      "Test Loss: 7.290426583494991e-05\n",
      "\n",
      "Epoch: 376\n",
      "Training Loss: 0.0004084073007106781\n",
      "Test Loss: 7.08602528902702e-05\n",
      "\n",
      "Epoch: 377\n",
      "Training Loss: 0.0003970917799354841\n",
      "Test Loss: 6.884735921630636e-05\n",
      "\n",
      "Epoch: 378\n",
      "Training Loss: 0.0003861267662917574\n",
      "Test Loss: 6.69031505822204e-05\n",
      "\n",
      "Epoch: 379\n",
      "Training Loss: 0.0003736289800144732\n",
      "Test Loss: 6.502322503365576e-05\n",
      "\n",
      "Epoch: 380\n",
      "Training Loss: 0.0003622856068735321\n",
      "Test Loss: 6.316122744465247e-05\n",
      "\n",
      "Epoch: 381\n",
      "Training Loss: 0.0003540074879614015\n",
      "Test Loss: 6.135340663604438e-05\n",
      "\n",
      "Epoch: 382\n",
      "Training Loss: 0.00034366231799746555\n",
      "Test Loss: 5.961219358141534e-05\n",
      "\n",
      "Epoch: 383\n",
      "Training Loss: 0.0003327467808655153\n",
      "Test Loss: 5.787583359051496e-05\n",
      "\n",
      "Epoch: 384\n",
      "Training Loss: 0.00032230250265759725\n",
      "Test Loss: 5.62082277610898e-05\n",
      "\n",
      "Epoch: 385\n",
      "Training Loss: 0.000313434002843375\n",
      "Test Loss: 5.4557145631406456e-05\n",
      "\n",
      "Epoch: 386\n",
      "Training Loss: 0.00030449135617042583\n",
      "Test Loss: 5.2949315431760624e-05\n",
      "\n",
      "Epoch: 387\n",
      "Training Loss: 0.00029555333700651926\n",
      "Test Loss: 5.141275323694572e-05\n",
      "\n",
      "Epoch: 388\n",
      "Training Loss: 0.0002864844282157719\n",
      "Test Loss: 4.989045555703342e-05\n",
      "\n",
      "Epoch: 389\n",
      "Training Loss: 0.0002789358259178698\n",
      "Test Loss: 4.8422967665828764e-05\n",
      "\n",
      "Epoch: 390\n",
      "Training Loss: 0.000269370706519112\n",
      "Test Loss: 4.6978751925053075e-05\n",
      "\n",
      "Epoch: 391\n",
      "Training Loss: 0.00025788725664218265\n",
      "Test Loss: 4.5570748625323176e-05\n",
      "\n",
      "Epoch: 392\n",
      "Training Loss: 0.0002526474854676053\n",
      "Test Loss: 4.422545316629112e-05\n",
      "\n",
      "Epoch: 393\n",
      "Training Loss: 0.0002456211077515036\n",
      "Test Loss: 4.2892421333817765e-05\n",
      "\n",
      "Epoch: 394\n",
      "Training Loss: 0.0002374654965630422\n",
      "Test Loss: 4.159535819781013e-05\n",
      "\n",
      "Epoch: 395\n",
      "Training Loss: 0.00023045031412038952\n",
      "Test Loss: 4.041146894451231e-05\n",
      "\n",
      "Epoch: 396\n",
      "Training Loss: 0.00022364402442084005\n",
      "Test Loss: 3.910554005415179e-05\n",
      "\n",
      "Epoch: 397\n",
      "Training Loss: 0.0002156867461356645\n",
      "Test Loss: 3.798892430495471e-05\n",
      "\n",
      "Epoch: 398\n",
      "Training Loss: 0.00020894969929940999\n",
      "Test Loss: 3.673667015391402e-05\n",
      "\n",
      "Epoch: 399\n",
      "Training Loss: 0.00020234108281632265\n",
      "Test Loss: 3.559965261956677e-05\n",
      "\n",
      "Epoch: 400\n",
      "Training Loss: 0.0001966857525985688\n",
      "Test Loss: 3.449094947427511e-05\n",
      "\n",
      "Epoch: 401\n",
      "Training Loss: 0.00018938495971572897\n",
      "Test Loss: 3.34168944391422e-05\n",
      "\n",
      "Epoch: 402\n",
      "Training Loss: 0.00018423596338834614\n",
      "Test Loss: 3.236677366658114e-05\n",
      "\n",
      "Epoch: 403\n",
      "Training Loss: 0.00017816257604863495\n",
      "Test Loss: 3.132779966108501e-05\n",
      "\n",
      "Epoch: 404\n",
      "Training Loss: 0.0001724261480073134\n",
      "Test Loss: 3.0340885132318363e-05\n",
      "\n",
      "Epoch: 405\n",
      "Training Loss: 0.0001662644741979117\n",
      "Test Loss: 2.9382779757725075e-05\n",
      "\n",
      "Epoch: 406\n",
      "Training Loss: 0.00016120895937395593\n",
      "Test Loss: 2.842885078280233e-05\n",
      "\n",
      "Epoch: 407\n",
      "Training Loss: 0.00015642613774010292\n",
      "Test Loss: 2.7539303118828684e-05\n",
      "\n",
      "Epoch: 408\n",
      "Training Loss: 0.00015047314809635282\n",
      "Test Loss: 2.6630992579157464e-05\n",
      "\n",
      "Epoch: 409\n",
      "Training Loss: 0.0001460860997516041\n",
      "Test Loss: 2.5768344130483456e-05\n",
      "\n",
      "Epoch: 410\n",
      "Training Loss: 0.00014110552244043598\n",
      "Test Loss: 2.4941438823589124e-05\n",
      "\n",
      "Epoch: 411\n",
      "Training Loss: 0.00013576300746838874\n",
      "Test Loss: 2.4121824026224203e-05\n",
      "\n",
      "Epoch: 412\n",
      "Training Loss: 0.00013178364800599715\n",
      "Test Loss: 2.333549127797596e-05\n",
      "\n",
      "Epoch: 413\n",
      "Training Loss: 0.00012720316832807535\n",
      "Test Loss: 2.2562733647646382e-05\n",
      "\n",
      "Epoch: 414\n",
      "Training Loss: 0.00012259888899279758\n",
      "Test Loss: 2.1818015738972463e-05\n",
      "\n",
      "Epoch: 415\n",
      "Training Loss: 0.00011888222312942769\n",
      "Test Loss: 2.1110936359036714e-05\n",
      "\n",
      "Epoch: 416\n",
      "Training Loss: 0.00011484527203720063\n",
      "Test Loss: 2.0393727027112618e-05\n",
      "\n",
      "Epoch: 417\n",
      "Training Loss: 0.00011061248854578783\n",
      "Test Loss: 1.9739596609724686e-05\n",
      "\n",
      "Epoch: 418\n",
      "Training Loss: 0.00010719295581414674\n",
      "Test Loss: 1.9071068891207688e-05\n",
      "\n",
      "Epoch: 419\n",
      "Training Loss: 0.00010336279471327241\n",
      "Test Loss: 1.8443392036715522e-05\n",
      "\n",
      "Epoch: 420\n",
      "Training Loss: 9.994660892213385e-05\n",
      "Test Loss: 1.7823043890530244e-05\n",
      "\n",
      "Epoch: 421\n",
      "Training Loss: 9.613529255148023e-05\n",
      "Test Loss: 1.7222049791598693e-05\n",
      "\n",
      "Epoch: 422\n",
      "Training Loss: 9.304867853643373e-05\n",
      "Test Loss: 1.6639438399579376e-05\n",
      "\n",
      "Epoch: 423\n",
      "Training Loss: 8.985292758249368e-05\n",
      "Test Loss: 1.6080422938102856e-05\n",
      "\n",
      "Epoch: 424\n",
      "Training Loss: 8.6320952201883e-05\n",
      "Test Loss: 1.553125366626773e-05\n",
      "\n",
      "Epoch: 425\n",
      "Training Loss: 8.373335731448606e-05\n",
      "Test Loss: 1.5002228792582173e-05\n",
      "\n",
      "Epoch: 426\n",
      "Training Loss: 8.035519810315843e-05\n",
      "Test Loss: 1.449042702006409e-05\n",
      "\n",
      "Epoch: 427\n",
      "Training Loss: 7.774053180279832e-05\n",
      "Test Loss: 1.3990963452670258e-05\n",
      "\n",
      "Epoch: 428\n",
      "Training Loss: 7.517945535558586e-05\n",
      "Test Loss: 1.3526881048164796e-05\n",
      "\n",
      "Epoch: 429\n",
      "Training Loss: 7.24278991886725e-05\n",
      "Test Loss: 1.3047864740656223e-05\n",
      "\n",
      "Epoch: 430\n",
      "Training Loss: 6.968065038866673e-05\n",
      "Test Loss: 1.2588042409333866e-05\n",
      "\n",
      "Epoch: 431\n",
      "Training Loss: 6.7235867997321e-05\n",
      "Test Loss: 1.2156125194451306e-05\n",
      "\n",
      "Epoch: 432\n",
      "Training Loss: 6.448222969387037e-05\n",
      "Test Loss: 1.1729410289262887e-05\n",
      "\n",
      "Epoch: 433\n",
      "Training Loss: 6.239766298676841e-05\n",
      "Test Loss: 1.1329988410579972e-05\n",
      "\n",
      "Epoch: 434\n",
      "Training Loss: 6.020497554951968e-05\n",
      "Test Loss: 1.0926902177743614e-05\n",
      "\n",
      "Epoch: 435\n",
      "Training Loss: 5.804244938190095e-05\n",
      "Test Loss: 1.0538926289882511e-05\n",
      "\n",
      "Epoch: 436\n",
      "Training Loss: 5.569264249061234e-05\n",
      "Test Loss: 1.0172921065532137e-05\n",
      "\n",
      "Epoch: 437\n",
      "Training Loss: 5.373657647093447e-05\n",
      "Test Loss: 9.820511877478566e-06\n",
      "\n",
      "Epoch: 438\n",
      "Training Loss: 5.177399604387271e-05\n",
      "Test Loss: 9.506150490778964e-06\n",
      "\n",
      "Epoch: 439\n",
      "Training Loss: 4.990876307905031e-05\n",
      "Test Loss: 9.187955583911389e-06\n",
      "\n",
      "Epoch: 440\n",
      "Training Loss: 4.8211200919467956e-05\n",
      "Test Loss: 8.849940059008077e-06\n",
      "\n",
      "Epoch: 441\n",
      "Training Loss: 4.63371676839112e-05\n",
      "Test Loss: 8.528524631401524e-06\n",
      "\n",
      "Epoch: 442\n",
      "Training Loss: 4.4437998440116644e-05\n",
      "Test Loss: 8.218138646043371e-06\n",
      "\n",
      "Epoch: 443\n",
      "Training Loss: 4.293736856197938e-05\n",
      "Test Loss: 7.923369594209362e-06\n",
      "\n",
      "Epoch: 444\n",
      "Training Loss: 4.130994663379776e-05\n",
      "Test Loss: 7.661264135094825e-06\n",
      "\n",
      "Epoch: 445\n",
      "Training Loss: 3.971234764321707e-05\n",
      "Test Loss: 7.374665528914193e-06\n",
      "\n",
      "Epoch: 446\n",
      "Training Loss: 3.820819862691375e-05\n",
      "Test Loss: 7.110944807209307e-06\n",
      "\n",
      "Epoch: 447\n",
      "Training Loss: 3.6824819593069456e-05\n",
      "Test Loss: 6.841643880761694e-06\n",
      "\n",
      "Epoch: 448\n",
      "Training Loss: 3.533226724054354e-05\n",
      "Test Loss: 6.597847459488548e-06\n",
      "\n",
      "Epoch: 449\n",
      "Training Loss: 3.402020956855267e-05\n",
      "Test Loss: 6.3424358813790604e-06\n",
      "\n",
      "Epoch: 450\n",
      "Training Loss: 3.2750186316358544e-05\n",
      "Test Loss: 6.112833943916485e-06\n",
      "\n",
      "Epoch: 451\n",
      "Training Loss: 3.1486878773042314e-05\n",
      "Test Loss: 5.893943125556689e-06\n",
      "\n",
      "Epoch: 452\n",
      "Training Loss: 3.023849118714376e-05\n",
      "Test Loss: 5.677940407622373e-06\n",
      "\n",
      "Epoch: 453\n",
      "Training Loss: 2.9090528793555375e-05\n",
      "Test Loss: 5.4753363656345755e-06\n",
      "\n",
      "Epoch: 454\n",
      "Training Loss: 2.779183099240375e-05\n",
      "Test Loss: 5.257376415102044e-06\n",
      "\n",
      "Epoch: 455\n",
      "Training Loss: 2.6788675919912446e-05\n",
      "Test Loss: 5.078281901660375e-06\n",
      "\n",
      "Epoch: 456\n",
      "Training Loss: 2.5777664632187225e-05\n",
      "Test Loss: 4.925616849504877e-06\n",
      "\n",
      "Epoch: 457\n",
      "Training Loss: 2.47782457639308e-05\n",
      "Test Loss: 4.713100679509807e-06\n",
      "\n",
      "Epoch: 458\n",
      "Training Loss: 2.376060971679787e-05\n",
      "Test Loss: 4.542343958746642e-06\n",
      "\n",
      "Epoch: 459\n",
      "Training Loss: 2.289624232313751e-05\n",
      "Test Loss: 4.372700914245797e-06\n",
      "\n",
      "Epoch: 460\n",
      "Training Loss: 2.1981589573745925e-05\n",
      "Test Loss: 4.2427918742760085e-06\n",
      "\n",
      "Epoch: 461\n",
      "Training Loss: 2.105148936000963e-05\n",
      "Test Loss: 4.027938757644733e-06\n",
      "\n",
      "Epoch: 462\n",
      "Training Loss: 2.0268034252997797e-05\n",
      "Test Loss: 3.910700343112694e-06\n",
      "\n",
      "Epoch: 463\n",
      "Training Loss: 1.9414574732460704e-05\n",
      "Test Loss: 3.761742391361622e-06\n",
      "\n",
      "Epoch: 464\n",
      "Training Loss: 1.865941097397202e-05\n",
      "Test Loss: 3.6263259062252473e-06\n",
      "\n",
      "Epoch: 465\n",
      "Training Loss: 1.791495439344241e-05\n",
      "Test Loss: 3.4924062219943153e-06\n",
      "\n",
      "Epoch: 466\n",
      "Training Loss: 1.7220790444601636e-05\n",
      "Test Loss: 3.396379952391726e-06\n",
      "\n",
      "Epoch: 467\n",
      "Training Loss: 1.6377428134243626e-05\n",
      "Test Loss: 3.2391035347245634e-06\n",
      "\n",
      "Epoch: 468\n",
      "Training Loss: 1.5852100659685675e-05\n",
      "Test Loss: 3.1176177799352445e-06\n",
      "\n",
      "Epoch: 469\n",
      "Training Loss: 1.5189625931573877e-05\n",
      "Test Loss: 3.00583997159265e-06\n",
      "\n",
      "Epoch: 470\n",
      "Training Loss: 1.4578774729064511e-05\n",
      "Test Loss: 2.893080136345816e-06\n",
      "\n",
      "Epoch: 471\n",
      "Training Loss: 1.397156787182515e-05\n",
      "Test Loss: 2.7567662073124666e-06\n",
      "\n",
      "Epoch: 472\n",
      "Training Loss: 1.337094909104053e-05\n",
      "Test Loss: 2.67825748778705e-06\n",
      "\n",
      "Epoch: 473\n",
      "Training Loss: 1.2841597405592134e-05\n",
      "Test Loss: 2.6639688712748466e-06\n",
      "\n",
      "Epoch: 474\n",
      "Training Loss: 1.2223892554175109e-05\n",
      "Test Loss: 2.4886267055990174e-06\n",
      "\n",
      "Epoch: 475\n",
      "Training Loss: 1.1826143842578555e-05\n",
      "Test Loss: 2.3891773253126303e-06\n",
      "\n",
      "Epoch: 476\n",
      "Training Loss: 1.1354996786394622e-05\n",
      "Test Loss: 2.3011677967588184e-06\n",
      "\n",
      "Epoch: 477\n",
      "Training Loss: 1.0877541778124092e-05\n",
      "Test Loss: 2.2161659671837697e-06\n",
      "\n",
      "Epoch: 478\n",
      "Training Loss: 1.0429236681375187e-05\n",
      "Test Loss: 2.1271189325489104e-06\n",
      "\n",
      "Epoch: 479\n",
      "Training Loss: 9.986718396248762e-06\n",
      "Test Loss: 2.054040123766754e-06\n",
      "\n",
      "Epoch: 480\n",
      "Training Loss: 9.566298407056214e-06\n",
      "Test Loss: 1.977731471924926e-06\n",
      "\n",
      "Epoch: 481\n",
      "Training Loss: 9.161042423026325e-06\n",
      "Test Loss: 1.9030985640711151e-06\n",
      "\n",
      "Epoch: 482\n",
      "Training Loss: 8.743412460413916e-06\n",
      "Test Loss: 1.8271693988936022e-06\n",
      "\n",
      "Epoch: 483\n",
      "Training Loss: 8.404211257584393e-06\n",
      "Test Loss: 1.7655935380389565e-06\n",
      "\n",
      "Epoch: 484\n",
      "Training Loss: 8.065099488400543e-06\n",
      "Test Loss: 1.6964149835985154e-06\n",
      "\n",
      "Epoch: 485\n",
      "Training Loss: 7.72435881420582e-06\n",
      "Test Loss: 1.6352267948605004e-06\n",
      "\n",
      "Epoch: 486\n",
      "Training Loss: 7.379321080710118e-06\n",
      "Test Loss: 1.5775142401253106e-06\n",
      "\n",
      "Epoch: 487\n",
      "Training Loss: 7.042576726234984e-06\n",
      "Test Loss: 1.535319256618095e-06\n",
      "\n",
      "Epoch: 488\n",
      "Training Loss: 6.775193014618708e-06\n",
      "Test Loss: 1.4666729839518666e-06\n",
      "\n",
      "Epoch: 489\n",
      "Training Loss: 6.4850103929832885e-06\n",
      "Test Loss: 1.4062917443879996e-06\n",
      "\n",
      "Epoch: 490\n",
      "Training Loss: 6.2249613013894605e-06\n",
      "Test Loss: 1.354581968371349e-06\n",
      "\n",
      "Epoch: 491\n",
      "Training Loss: 5.918503120483365e-06\n",
      "Test Loss: 1.303437898059201e-06\n",
      "\n",
      "Epoch: 492\n",
      "Training Loss: 5.682649316440802e-06\n",
      "Test Loss: 1.272480517400254e-06\n",
      "\n",
      "Epoch: 493\n",
      "Training Loss: 5.440992026706226e-06\n",
      "Test Loss: 1.2092741599190049e-06\n",
      "\n",
      "Epoch: 494\n",
      "Training Loss: 5.216423384505712e-06\n",
      "Test Loss: 1.1653254432530957e-06\n",
      "\n",
      "Epoch: 495\n",
      "Training Loss: 4.9865817951892195e-06\n",
      "Test Loss: 1.1221917475268128e-06\n",
      "\n",
      "Epoch: 496\n",
      "Training Loss: 4.769685347127961e-06\n",
      "Test Loss: 1.080935362551827e-06\n",
      "\n",
      "Epoch: 497\n",
      "Training Loss: 4.553422816873838e-06\n",
      "Test Loss: 1.0397482128610136e-06\n",
      "\n",
      "Epoch: 498\n",
      "Training Loss: 4.343979071563808e-06\n",
      "Test Loss: 1.0075020782096544e-06\n",
      "\n",
      "Epoch: 499\n",
      "Training Loss: 4.1606806462368695e-06\n",
      "Test Loss: 9.686960993349203e-07\n",
      "\n",
      "Epoch: 500\n",
      "Training Loss: 3.9862805654896265e-06\n",
      "Test Loss: 9.332108561466157e-07\n",
      "\n",
      "Epoch: 501\n",
      "Training Loss: 3.829403719161443e-06\n",
      "Test Loss: 8.997164400170732e-07\n",
      "\n",
      "Epoch: 502\n",
      "Training Loss: 3.647551769366449e-06\n",
      "Test Loss: 8.675326057527855e-07\n",
      "\n",
      "Epoch: 503\n",
      "Training Loss: 3.4978276441203584e-06\n",
      "Test Loss: 8.350411917490419e-07\n",
      "\n",
      "Epoch: 504\n",
      "Training Loss: 3.3170852778615276e-06\n",
      "Test Loss: 8.03958016604156e-07\n",
      "\n",
      "Epoch: 505\n",
      "Training Loss: 3.1786887575435685e-06\n",
      "Test Loss: 7.743807373117306e-07\n",
      "\n",
      "Epoch: 506\n",
      "Training Loss: 3.0462085002606423e-06\n",
      "Test Loss: 7.463062274837284e-07\n",
      "\n",
      "Epoch: 507\n",
      "Training Loss: 2.897633673152692e-06\n",
      "Test Loss: 7.189527764239756e-07\n",
      "\n",
      "Epoch: 508\n",
      "Training Loss: 2.779866311660347e-06\n",
      "Test Loss: 6.903741223140969e-07\n",
      "\n",
      "Epoch: 509\n",
      "Training Loss: 2.6450015866430476e-06\n",
      "Test Loss: 6.650881800851494e-07\n",
      "\n",
      "Epoch: 510\n",
      "Training Loss: 2.539795787015464e-06\n",
      "Test Loss: 6.393465241671947e-07\n",
      "\n",
      "Epoch: 511\n",
      "Training Loss: 2.4369512630073586e-06\n",
      "Test Loss: 6.205873432918452e-07\n",
      "\n",
      "Epoch: 512\n",
      "Training Loss: 2.3087553320995844e-06\n",
      "Test Loss: 5.976148145236948e-07\n",
      "\n",
      "Epoch: 513\n",
      "Training Loss: 2.2089321115951557e-06\n",
      "Test Loss: 5.75913361444691e-07\n",
      "\n",
      "Epoch: 514\n",
      "Training Loss: 2.1249663859634893e-06\n",
      "Test Loss: 5.559512032959901e-07\n",
      "\n",
      "Epoch: 515\n",
      "Training Loss: 2.028399421760696e-06\n",
      "Test Loss: 5.463389811666275e-07\n",
      "\n",
      "Epoch: 516\n",
      "Training Loss: 1.932194891196559e-06\n",
      "Test Loss: 5.170751364858006e-07\n",
      "\n",
      "Epoch: 517\n",
      "Training Loss: 1.8426663170127238e-06\n",
      "Test Loss: 5.012427095607563e-07\n",
      "\n",
      "Epoch: 518\n",
      "Training Loss: 1.7590544227156595e-06\n",
      "Test Loss: 5.023997573516681e-07\n",
      "\n",
      "Epoch: 519\n",
      "Training Loss: 1.6842208575932698e-06\n",
      "Test Loss: 4.680352674313326e-07\n",
      "\n",
      "Epoch: 520\n",
      "Training Loss: 1.5974372521062226e-06\n",
      "Test Loss: 4.5810378423993825e-07\n",
      "\n",
      "Epoch: 521\n",
      "Training Loss: 1.5263548220900702e-06\n",
      "Test Loss: 4.6219335558816965e-07\n",
      "\n",
      "Epoch: 522\n",
      "Training Loss: 1.4700026440550573e-06\n",
      "Test Loss: 4.288352215553459e-07\n",
      "\n",
      "Epoch: 523\n",
      "Training Loss: 1.394396955826475e-06\n",
      "Test Loss: 4.088142020464147e-07\n",
      "\n",
      "Epoch: 524\n",
      "Training Loss: 1.3378862983396782e-06\n",
      "Test Loss: 4.1177062826136535e-07\n",
      "\n",
      "Epoch: 525\n",
      "Training Loss: 1.2775685339268723e-06\n",
      "Test Loss: 3.8606921748396417e-07\n",
      "\n",
      "Epoch: 526\n",
      "Training Loss: 1.215963114494419e-06\n",
      "Test Loss: 3.753239639081585e-07\n",
      "\n",
      "Epoch: 527\n",
      "Training Loss: 1.1625864241674815e-06\n",
      "Test Loss: 3.642193178166053e-07\n",
      "\n",
      "Epoch: 528\n",
      "Training Loss: 1.1089584480335664e-06\n",
      "Test Loss: 3.49903643837024e-07\n",
      "\n",
      "Epoch: 529\n",
      "Training Loss: 1.0550679310957396e-06\n",
      "Test Loss: 3.3957013556573656e-07\n",
      "\n",
      "Epoch: 530\n",
      "Training Loss: 1.0087615388935471e-06\n",
      "Test Loss: 3.2903287205954257e-07\n",
      "\n",
      "Epoch: 531\n",
      "Training Loss: 9.618717058401671e-07\n",
      "Test Loss: 3.2015447004596354e-07\n",
      "\n",
      "Epoch: 532\n",
      "Training Loss: 9.154182597133816e-07\n",
      "Test Loss: 3.1139379075284523e-07\n",
      "\n",
      "Epoch: 533\n",
      "Training Loss: 8.693718882568646e-07\n",
      "Test Loss: 3.0210111390260863e-07\n",
      "\n",
      "Epoch: 534\n",
      "Training Loss: 8.342048545273428e-07\n",
      "Test Loss: 2.940458045941341e-07\n",
      "\n",
      "Epoch: 535\n",
      "Training Loss: 7.991271786522702e-07\n",
      "Test Loss: 2.85910488173613e-07\n",
      "\n",
      "Epoch: 536\n",
      "Training Loss: 7.583918962457877e-07\n",
      "Test Loss: 2.778975840556086e-07\n",
      "\n",
      "Epoch: 537\n",
      "Training Loss: 7.255485456880706e-07\n",
      "Test Loss: 2.7000641011909465e-07\n",
      "\n",
      "Epoch: 538\n",
      "Training Loss: 6.918888099486745e-07\n",
      "Test Loss: 2.6328723379265284e-07\n",
      "\n",
      "Epoch: 539\n",
      "Training Loss: 6.598441662693707e-07\n",
      "Test Loss: 2.560189216183062e-07\n",
      "\n",
      "Epoch: 540\n",
      "Training Loss: 6.298500162908264e-07\n",
      "Test Loss: 2.4998266212605813e-07\n",
      "\n",
      "Epoch: 541\n",
      "Training Loss: 6.011846949149913e-07\n",
      "Test Loss: 2.412185722278082e-07\n",
      "\n",
      "Epoch: 542\n",
      "Training Loss: 5.718291617995419e-07\n",
      "Test Loss: 2.377052226165688e-07\n",
      "\n",
      "Epoch: 543\n",
      "Training Loss: 5.476200802452998e-07\n",
      "Test Loss: 2.321068848232244e-07\n",
      "\n",
      "Epoch: 544\n",
      "Training Loss: 5.236861397861503e-07\n",
      "Test Loss: 2.2725454584815452e-07\n",
      "\n",
      "Epoch: 545\n",
      "Training Loss: 4.989383626252675e-07\n",
      "Test Loss: 2.2139778366181417e-07\n",
      "\n",
      "Epoch: 546\n",
      "Training Loss: 4.771038201549042e-07\n",
      "Test Loss: 2.158521823503179e-07\n",
      "\n",
      "Epoch: 547\n",
      "Training Loss: 4.5432320424273104e-07\n",
      "Test Loss: 2.1125188709447684e-07\n",
      "\n",
      "Epoch: 548\n",
      "Training Loss: 4.3375525630532746e-07\n",
      "Test Loss: 2.0630153585443622e-07\n",
      "\n",
      "Epoch: 549\n",
      "Training Loss: 4.1395404082322784e-07\n",
      "Test Loss: 2.0639146214307402e-07\n",
      "\n",
      "Epoch: 550\n",
      "Training Loss: 3.947314534495187e-07\n",
      "Test Loss: 1.9629402459031553e-07\n",
      "\n",
      "Epoch: 551\n",
      "Training Loss: 3.7557365809940774e-07\n",
      "Test Loss: 1.940683205248206e-07\n",
      "\n",
      "Epoch: 552\n",
      "Training Loss: 3.599282649702218e-07\n",
      "Test Loss: 1.9058916223002598e-07\n",
      "\n",
      "Epoch: 553\n",
      "Training Loss: 3.4379550584162644e-07\n",
      "Test Loss: 1.8678636592994735e-07\n",
      "\n",
      "Epoch: 554\n",
      "Training Loss: 3.30377702084661e-07\n",
      "Test Loss: 1.8334944229536632e-07\n",
      "\n",
      "Epoch: 555\n",
      "Training Loss: 3.145768895744065e-07\n",
      "Test Loss: 1.7968129384371423e-07\n",
      "\n",
      "Epoch: 556\n",
      "Training Loss: 3.003206643370504e-07\n",
      "Test Loss: 1.7573226784861617e-07\n",
      "\n",
      "Epoch: 557\n",
      "Training Loss: 2.9342884507362516e-07\n",
      "Test Loss: 1.7794788220726332e-07\n",
      "\n",
      "Epoch: 558\n",
      "Training Loss: 2.758415860171226e-07\n",
      "Test Loss: 1.720363655977053e-07\n",
      "\n",
      "Epoch: 559\n",
      "Training Loss: 2.621972801838031e-07\n",
      "Test Loss: 1.7018291487147508e-07\n",
      "\n",
      "Epoch: 560\n",
      "Training Loss: 2.5301272899014293e-07\n",
      "Test Loss: 1.6636883515275258e-07\n",
      "\n",
      "Epoch: 561\n",
      "Training Loss: 2.388650888936657e-07\n",
      "Test Loss: 1.5986353218977456e-07\n",
      "\n",
      "Epoch: 562\n",
      "Training Loss: 2.287762868036225e-07\n",
      "Test Loss: 1.6103081179608125e-07\n",
      "\n",
      "Epoch: 563\n",
      "Training Loss: 2.1946225577570053e-07\n",
      "Test Loss: 1.562580393965618e-07\n",
      "\n",
      "Epoch: 564\n",
      "Training Loss: 2.0839827167643912e-07\n",
      "Test Loss: 1.5322464719247364e-07\n",
      "\n",
      "Epoch: 565\n",
      "Training Loss: 1.9925569461065606e-07\n",
      "Test Loss: 1.496833874625736e-07\n",
      "\n",
      "Epoch: 566\n",
      "Training Loss: 1.9014332034809436e-07\n",
      "Test Loss: 1.4698647987643199e-07\n",
      "\n",
      "Epoch: 567\n",
      "Training Loss: 1.8358699567973721e-07\n",
      "Test Loss: 1.4507644152672583e-07\n",
      "\n",
      "Epoch: 568\n",
      "Training Loss: 1.754225327961952e-07\n",
      "Test Loss: 1.4312273322047986e-07\n",
      "\n",
      "Epoch: 569\n",
      "Training Loss: 1.7564026203823838e-07\n",
      "Test Loss: 1.425056268544722e-07\n",
      "\n",
      "Epoch: 570\n",
      "Training Loss: 1.6254334411769378e-07\n",
      "Test Loss: 1.4438106177294685e-07\n",
      "\n",
      "Epoch: 571\n",
      "Training Loss: 1.568297894512701e-07\n",
      "Test Loss: 1.38875506650038e-07\n",
      "\n",
      "Epoch: 572\n",
      "Training Loss: 1.4924273254261303e-07\n",
      "Test Loss: 1.3542351950945886e-07\n",
      "\n",
      "Epoch: 573\n",
      "Training Loss: 1.4186556522114793e-07\n",
      "Test Loss: 1.3244343222140742e-07\n",
      "\n",
      "Epoch: 574\n",
      "Training Loss: 1.363648844214064e-07\n",
      "Test Loss: 1.5851776424824493e-07\n",
      "\n",
      "Epoch: 575\n",
      "Training Loss: 1.3330992961376373e-07\n",
      "Test Loss: 1.2997261933378468e-07\n",
      "\n",
      "Epoch: 576\n",
      "Training Loss: 1.2542430975296762e-07\n",
      "Test Loss: 1.271689740178772e-07\n",
      "\n",
      "Epoch: 577\n",
      "Training Loss: 1.1983374056020088e-07\n",
      "Test Loss: 1.2616206390703155e-07\n",
      "\n",
      "Epoch: 578\n",
      "Training Loss: 1.2404845506353013e-07\n",
      "Test Loss: 1.323370213413e-07\n",
      "\n",
      "Epoch: 579\n",
      "Training Loss: 1.1553017031928903e-07\n",
      "Test Loss: 1.2989058006951382e-07\n",
      "\n",
      "Epoch: 580\n",
      "Training Loss: 1.0786335972549448e-07\n",
      "Test Loss: 1.2193700626994541e-07\n",
      "\n",
      "Epoch: 581\n",
      "Training Loss: 1.0156147330538563e-07\n",
      "Test Loss: 1.2040555930070695e-07\n",
      "\n",
      "Epoch: 582\n",
      "Training Loss: 9.969780497461518e-08\n",
      "Test Loss: 1.2416030870099348e-07\n",
      "\n",
      "Epoch: 583\n",
      "Training Loss: 1.0001729814727393e-07\n",
      "Test Loss: 1.390427399883265e-07\n",
      "\n",
      "Epoch: 584\n",
      "Training Loss: 9.870542783119163e-08\n",
      "Test Loss: 1.1871797767071257e-07\n",
      "\n",
      "Epoch: 585\n",
      "Training Loss: 9.023132937121166e-08\n",
      "Test Loss: 1.1271912825350228e-07\n",
      "\n",
      "Epoch: 586\n",
      "Training Loss: 8.86639692036321e-08\n",
      "Test Loss: 1.1415457379371219e-07\n",
      "\n",
      "Epoch: 587\n",
      "Training Loss: 8.447595452783692e-08\n",
      "Test Loss: 2.0976396797323105e-07\n",
      "\n",
      "Epoch: 588\n",
      "Training Loss: 9.294184621921886e-08\n",
      "Test Loss: 2.1566715702192596e-07\n",
      "\n",
      "Epoch: 589\n",
      "Training Loss: 8.837983500598057e-08\n",
      "Test Loss: 1.230041561939288e-07\n",
      "\n",
      "Epoch: 590\n",
      "Training Loss: 7.487421764077833e-08\n",
      "Test Loss: 1.1333619198694578e-07\n",
      "\n",
      "Epoch: 591\n",
      "Training Loss: 7.34440869602319e-08\n",
      "Test Loss: 1.1375709618732799e-07\n",
      "\n",
      "Epoch: 592\n",
      "Training Loss: 7.50047680260953e-08\n",
      "Test Loss: 1.1299819391297206e-07\n",
      "\n",
      "Epoch: 593\n",
      "Training Loss: 6.86978485949415e-08\n",
      "Test Loss: 1.1542275046849682e-07\n",
      "\n",
      "Epoch: 594\n",
      "Training Loss: 6.72068978246898e-08\n",
      "Test Loss: 1.1184402382014014e-07\n",
      "\n",
      "Epoch: 595\n",
      "Training Loss: 6.258551129197561e-08\n",
      "Test Loss: 1.0507271497317561e-07\n",
      "\n",
      "Epoch: 596\n",
      "Training Loss: 6.027212625288787e-08\n",
      "Test Loss: 1.0335046596310349e-07\n",
      "\n",
      "Epoch: 597\n",
      "Training Loss: 5.859664871839717e-08\n",
      "Test Loss: 1.0517052828618034e-07\n",
      "\n",
      "Epoch: 598\n",
      "Training Loss: 5.8599768001007156e-08\n",
      "Test Loss: 1.0864679467204041e-07\n",
      "\n",
      "Epoch: 599\n",
      "Training Loss: 6.25227845792627e-08\n",
      "Test Loss: 1.2259505410838756e-07\n",
      "\n",
      "Epoch: 600\n",
      "Training Loss: 6.846834329129099e-08\n",
      "Test Loss: 1.1334699223652933e-07\n",
      "\n",
      "Epoch: 601\n",
      "Training Loss: 6.264863354014476e-08\n",
      "Test Loss: 1.1367822594365862e-07\n",
      "\n",
      "Epoch: 602\n",
      "Training Loss: 5.314547261529393e-08\n",
      "Test Loss: 1.0299468300445369e-07\n",
      "\n",
      "Epoch: 603\n",
      "Training Loss: 5.035593394116707e-08\n",
      "Test Loss: 9.301563608232755e-08\n",
      "\n",
      "Epoch: 604\n",
      "Training Loss: 4.770418845131038e-08\n",
      "Test Loss: 9.462979733143584e-08\n",
      "\n",
      "Epoch: 605\n",
      "Training Loss: 4.842078027422758e-08\n",
      "Test Loss: 9.815481405439641e-08\n",
      "\n",
      "Epoch: 606\n",
      "Training Loss: 4.635458950019711e-08\n",
      "Test Loss: 9.280931578814489e-08\n",
      "\n",
      "Epoch: 607\n",
      "Training Loss: 4.450842302124632e-08\n",
      "Test Loss: 9.211418472432342e-08\n",
      "\n",
      "Epoch: 608\n",
      "Training Loss: 4.290055836501475e-08\n",
      "Test Loss: 9.079643348286481e-08\n",
      "\n",
      "Epoch: 609\n",
      "Training Loss: 4.157472825265055e-08\n",
      "Test Loss: 1.2843700858411466e-07\n",
      "\n",
      "Epoch: 610\n",
      "Training Loss: 4.3298047813777885e-08\n",
      "Test Loss: 9.003774437132961e-08\n",
      "\n",
      "Epoch: 611\n",
      "Training Loss: 3.9935031281856936e-08\n",
      "Test Loss: 8.98353533784757e-08\n",
      "\n",
      "Epoch: 612\n",
      "Training Loss: 3.9659024508864604e-08\n",
      "Test Loss: 8.97707153058036e-08\n",
      "\n",
      "Epoch: 613\n",
      "Training Loss: 3.986206564832173e-08\n",
      "Test Loss: 9.187208149796788e-08\n",
      "\n",
      "Epoch: 614\n",
      "Training Loss: 4.1044787716752275e-08\n",
      "Test Loss: 9.398453215681002e-08\n",
      "\n",
      "Epoch: 615\n",
      "Training Loss: 4.1983189665491714e-08\n",
      "Test Loss: 9.717729199110181e-08\n",
      "\n",
      "Epoch: 616\n",
      "Training Loss: 4.3643111051020846e-08\n",
      "Test Loss: 1.149589294868747e-07\n",
      "\n",
      "Epoch: 617\n",
      "Training Loss: 4.846127055202487e-08\n",
      "Test Loss: 1.0374735381901701e-07\n",
      "\n",
      "Epoch: 618\n",
      "Training Loss: 5.378704178345591e-08\n",
      "Test Loss: 9.65303001976281e-08\n",
      "\n",
      "Epoch: 619\n",
      "Training Loss: 3.659264417403089e-08\n",
      "Test Loss: 9.023467839597288e-08\n",
      "\n",
      "Epoch: 620\n",
      "Training Loss: 3.459742951387549e-08\n",
      "Test Loss: 8.66293063950252e-08\n",
      "\n",
      "Epoch: 621\n",
      "Training Loss: 3.324274189253629e-08\n",
      "Test Loss: 8.435387144345441e-08\n",
      "\n",
      "Epoch: 622\n",
      "Training Loss: 3.1275253794168144e-08\n",
      "Test Loss: 8.527914729938857e-08\n",
      "\n",
      "Epoch: 623\n",
      "Training Loss: 3.092913599308152e-08\n",
      "Test Loss: 8.690192743188163e-08\n",
      "\n",
      "Epoch: 624\n",
      "Training Loss: 3.417540680080341e-08\n",
      "Test Loss: 8.910613047419247e-08\n",
      "\n",
      "Epoch: 625\n",
      "Training Loss: 2.9258792129856676e-08\n",
      "Test Loss: 8.374422577617224e-08\n",
      "\n",
      "Epoch: 626\n",
      "Training Loss: 2.8087673224301096e-08\n",
      "Test Loss: 8.159281605912838e-08\n",
      "\n",
      "Epoch: 627\n",
      "Training Loss: 2.915196795072461e-08\n",
      "Test Loss: 8.09455826811245e-08\n",
      "\n",
      "Epoch: 628\n",
      "Training Loss: 2.7410740936299287e-08\n",
      "Test Loss: 7.979750904496541e-08\n",
      "\n",
      "Epoch: 629\n",
      "Training Loss: 2.8629905557409074e-08\n",
      "Test Loss: 8.195235778885035e-08\n",
      "\n",
      "Epoch: 630\n",
      "Training Loss: 2.736330214266521e-08\n",
      "Test Loss: 9.629940933564285e-08\n",
      "\n",
      "Epoch: 631\n",
      "Training Loss: 2.6822027858012614e-08\n",
      "Test Loss: 7.99356172365151e-08\n",
      "\n",
      "Epoch: 632\n",
      "Training Loss: 2.519455873558248e-08\n",
      "Test Loss: 7.884345620823296e-08\n",
      "\n",
      "Epoch: 633\n",
      "Training Loss: 2.5994315440887778e-08\n",
      "Test Loss: 7.88029126397305e-08\n",
      "\n",
      "Epoch: 634\n",
      "Training Loss: 2.6004237578073724e-08\n",
      "Test Loss: 7.958664127727388e-08\n",
      "\n",
      "Epoch: 635\n",
      "Training Loss: 2.3897642975612143e-08\n",
      "Test Loss: 7.843520677397464e-08\n",
      "\n",
      "Epoch: 636\n",
      "Training Loss: 2.3599026140648977e-08\n",
      "Test Loss: 7.784620947859366e-08\n",
      "\n",
      "Epoch: 637\n",
      "Training Loss: 2.3427499939998597e-08\n",
      "Test Loss: 7.757812880981874e-08\n",
      "\n",
      "Epoch: 638\n",
      "Training Loss: 2.2999607551810186e-08\n",
      "Test Loss: 7.746019292653727e-08\n",
      "\n",
      "Epoch: 639\n",
      "Training Loss: 2.2893754447750325e-08\n",
      "Test Loss: 7.69733219385671e-08\n",
      "\n",
      "Epoch: 640\n",
      "Training Loss: 2.263075652801187e-08\n",
      "Test Loss: 7.657295242324835e-08\n",
      "\n",
      "Epoch: 641\n",
      "Training Loss: 2.2765865637097704e-08\n",
      "Test Loss: 7.650670141856608e-08\n",
      "\n",
      "Epoch: 642\n",
      "Training Loss: 2.3018381867245807e-08\n",
      "Test Loss: 8.72303829169141e-08\n",
      "\n",
      "Epoch: 643\n",
      "Training Loss: 2.514410901710562e-08\n",
      "Test Loss: 7.893874709452575e-08\n",
      "\n",
      "Epoch: 644\n",
      "Training Loss: 2.5824157035003736e-08\n",
      "Test Loss: 7.953429559393044e-08\n",
      "\n",
      "Epoch: 645\n",
      "Training Loss: 2.766412343646607e-08\n",
      "Test Loss: 8.024380804272369e-08\n",
      "\n",
      "Epoch: 646\n",
      "Training Loss: 2.677703510774639e-08\n",
      "Test Loss: 7.688750258694199e-08\n",
      "\n",
      "Epoch: 647\n",
      "Training Loss: 2.235589668941884e-08\n",
      "Test Loss: 7.477773067421367e-08\n",
      "\n",
      "Epoch: 648\n",
      "Training Loss: 2.2251868495951992e-08\n",
      "Test Loss: 7.591095396719538e-08\n",
      "\n",
      "Epoch: 649\n",
      "Training Loss: 2.4462651756114912e-08\n",
      "Test Loss: 7.896086628988996e-08\n",
      "\n",
      "Epoch: 650\n",
      "Training Loss: 2.6030618253495657e-08\n",
      "Test Loss: 7.683888725296129e-08\n",
      "\n",
      "Epoch: 651\n",
      "Training Loss: 2.1895716647198544e-08\n",
      "Test Loss: 7.405978408314695e-08\n",
      "\n",
      "Epoch: 652\n",
      "Training Loss: 2.1052333210036522e-08\n",
      "Test Loss: 7.67072805274438e-08\n",
      "\n",
      "Epoch: 653\n",
      "Training Loss: 2.5264050999377712e-08\n",
      "Test Loss: 7.877041241499683e-08\n",
      "\n",
      "Epoch: 654\n",
      "Training Loss: 2.4618352985802023e-08\n",
      "Test Loss: 7.357053988243933e-08\n",
      "\n",
      "Epoch: 655\n",
      "Training Loss: 2.0242442981081393e-08\n",
      "Test Loss: 7.440830529503728e-08\n",
      "\n",
      "Epoch: 656\n",
      "Training Loss: 2.2350945390788485e-08\n",
      "Test Loss: 7.68303891618416e-08\n",
      "\n",
      "Epoch: 657\n",
      "Training Loss: 2.3609543949495066e-08\n",
      "Test Loss: 7.459450301894321e-08\n",
      "\n",
      "Epoch: 658\n",
      "Training Loss: 2.0349096629956875e-08\n",
      "Test Loss: 7.264854673394439e-08\n",
      "\n",
      "Epoch: 659\n",
      "Training Loss: 2.0011341324751204e-08\n",
      "Test Loss: 7.437445503910567e-08\n",
      "\n",
      "Epoch: 660\n",
      "Training Loss: 2.3259428161281903e-08\n",
      "Test Loss: 7.400875290386466e-08\n",
      "\n",
      "Epoch: 661\n",
      "Training Loss: 2.070147845737817e-08\n",
      "Test Loss: 7.13701240329101e-08\n",
      "\n",
      "Epoch: 662\n",
      "Training Loss: 1.8793196332467232e-08\n",
      "Test Loss: 7.229387932738973e-08\n",
      "\n",
      "Epoch: 663\n",
      "Training Loss: 2.7073327283725728e-08\n",
      "Test Loss: 8.132086293244356e-08\n",
      "\n",
      "Epoch: 664\n",
      "Training Loss: 2.08522739815938e-08\n",
      "Test Loss: 7.512949906640642e-08\n",
      "\n",
      "Epoch: 665\n",
      "Training Loss: 1.9160699693543393e-08\n",
      "Test Loss: 7.270089241728783e-08\n",
      "\n",
      "Epoch: 666\n",
      "Training Loss: 1.9232212859302915e-08\n",
      "Test Loss: 7.178910976790576e-08\n",
      "\n",
      "Epoch: 667\n",
      "Training Loss: 2.0256722521594384e-08\n",
      "Test Loss: 7.638379884156166e-08\n",
      "\n",
      "Epoch: 668\n",
      "Training Loss: 1.9338462683056907e-08\n",
      "Test Loss: 7.339684771068278e-08\n",
      "\n",
      "Epoch: 669\n",
      "Training Loss: 1.8642166329148797e-08\n",
      "Test Loss: 7.153641234936003e-08\n",
      "\n",
      "Epoch: 670\n",
      "Training Loss: 1.8314733433536883e-08\n",
      "Test Loss: 6.986417844245807e-08\n",
      "\n",
      "Epoch: 671\n",
      "Training Loss: 1.82088673028602e-08\n",
      "Test Loss: 7.103022170440454e-08\n",
      "\n",
      "Epoch: 672\n",
      "Training Loss: 1.8607031175103355e-08\n",
      "Test Loss: 7.018950753945319e-08\n",
      "\n",
      "Epoch: 673\n",
      "Training Loss: 1.8024379026352715e-08\n",
      "Test Loss: 6.875348645962731e-08\n",
      "\n",
      "Epoch: 674\n",
      "Training Loss: 1.778252804266837e-08\n",
      "Test Loss: 6.940982189007627e-08\n",
      "\n",
      "Epoch: 675\n",
      "Training Loss: 1.834896442195107e-08\n",
      "Test Loss: 8.442101062655638e-08\n",
      "\n",
      "Epoch: 676\n",
      "Training Loss: 1.9786707383673274e-08\n",
      "Test Loss: 1.203768107416181e-07\n",
      "\n",
      "Epoch: 677\n",
      "Training Loss: 2.4914766546165385e-08\n",
      "Test Loss: 7.012142333451266e-08\n",
      "\n",
      "Epoch: 678\n",
      "Training Loss: 1.817708946324122e-08\n",
      "Test Loss: 6.611621472529805e-08\n",
      "\n",
      "Epoch: 679\n",
      "Training Loss: 1.9185796655089387e-08\n",
      "Test Loss: 6.69206627890162e-08\n",
      "\n",
      "Epoch: 680\n",
      "Training Loss: 2.0292591903133445e-08\n",
      "Test Loss: 6.854097733821618e-08\n",
      "\n",
      "Epoch: 681\n",
      "Training Loss: 1.9307940727723388e-08\n",
      "Test Loss: 6.818331144131662e-08\n",
      "\n",
      "Epoch: 682\n",
      "Training Loss: 1.910194669108023e-08\n",
      "Test Loss: 6.720300405049784e-08\n",
      "\n",
      "Epoch: 683\n",
      "Training Loss: 1.905138535818196e-08\n",
      "Test Loss: 7.595749451638767e-08\n",
      "\n",
      "Epoch: 684\n",
      "Training Loss: 1.9937668701193918e-08\n",
      "Test Loss: 6.718914136172316e-08\n",
      "\n",
      "Epoch: 685\n",
      "Training Loss: 1.84998617906255e-08\n",
      "Test Loss: 6.59496777188906e-08\n",
      "\n",
      "Epoch: 686\n",
      "Training Loss: 1.835494423119144e-08\n",
      "Test Loss: 6.555268328156671e-08\n",
      "\n",
      "Epoch: 687\n",
      "Training Loss: 1.8327058389407586e-08\n",
      "Test Loss: 6.559636034353389e-08\n",
      "\n",
      "Epoch: 688\n",
      "Training Loss: 1.8119588792349834e-08\n",
      "Test Loss: 7.71707533431254e-08\n",
      "\n",
      "Epoch: 689\n",
      "Training Loss: 2.0378561652970955e-08\n",
      "Test Loss: 6.780597772149122e-08\n",
      "\n",
      "Epoch: 690\n",
      "Training Loss: 1.6403255938731338e-08\n",
      "Test Loss: 6.410715513993637e-08\n",
      "\n",
      "Epoch: 691\n",
      "Training Loss: 1.7192081832225387e-08\n",
      "Test Loss: 6.487039172498044e-08\n",
      "\n",
      "Epoch: 692\n",
      "Training Loss: 1.762659055752162e-08\n",
      "Test Loss: 6.632197369071946e-08\n",
      "\n",
      "Epoch: 693\n",
      "Training Loss: 1.816782398596691e-08\n",
      "Test Loss: 6.684132358714123e-08\n",
      "\n",
      "Epoch: 694\n",
      "Training Loss: 1.8022240292718077e-08\n",
      "Test Loss: 6.627819004734192e-08\n",
      "\n",
      "Epoch: 695\n",
      "Training Loss: 1.7781884705433033e-08\n",
      "Test Loss: 6.556459197781805e-08\n",
      "\n",
      "Epoch: 696\n",
      "Training Loss: 1.7439057013272457e-08\n",
      "Test Loss: 6.523654150214497e-08\n",
      "\n",
      "Epoch: 697\n",
      "Training Loss: 1.7218771297677904e-08\n",
      "Test Loss: 6.984536327081514e-08\n",
      "\n",
      "Epoch: 698\n",
      "Training Loss: 1.729362134976024e-08\n",
      "Test Loss: 6.386641615563349e-08\n",
      "\n",
      "Epoch: 699\n",
      "Training Loss: 1.6493691745722572e-08\n",
      "Test Loss: 6.379218575602863e-08\n",
      "\n",
      "Epoch: 700\n",
      "Training Loss: 1.6334078386156154e-08\n",
      "Test Loss: 6.342011715787521e-08\n",
      "\n",
      "Epoch: 701\n",
      "Training Loss: 1.6140859315783018e-08\n",
      "Test Loss: 1.1517882114731037e-07\n",
      "\n",
      "Epoch: 702\n",
      "Training Loss: 2.3343553460601168e-08\n",
      "Test Loss: 7.132789647812388e-08\n",
      "\n",
      "Epoch: 703\n",
      "Training Loss: 1.7045177713725934e-08\n",
      "Test Loss: 6.253334561279189e-08\n",
      "\n",
      "Epoch: 704\n",
      "Training Loss: 1.6071925124094832e-08\n",
      "Test Loss: 6.39788808598496e-08\n",
      "\n",
      "Epoch: 705\n",
      "Training Loss: 1.566489634304465e-08\n",
      "Test Loss: 6.214150971572963e-08\n",
      "\n",
      "Epoch: 706\n",
      "Training Loss: 1.5779378988630317e-08\n",
      "Test Loss: 6.434470378735568e-08\n",
      "\n",
      "Epoch: 707\n",
      "Training Loss: 1.5614014081677396e-08\n",
      "Test Loss: 6.310953892807447e-08\n",
      "\n",
      "Epoch: 708\n",
      "Training Loss: 1.5420928534126688e-08\n",
      "Test Loss: 6.397866059160151e-08\n",
      "\n",
      "Epoch: 709\n",
      "Training Loss: 1.6620851065833147e-08\n",
      "Test Loss: 6.396886931270274e-08\n",
      "\n",
      "Epoch: 710\n",
      "Training Loss: 1.726785647804263e-08\n",
      "Test Loss: 6.522322593127683e-08\n",
      "\n",
      "Epoch: 711\n",
      "Training Loss: 1.704186362398256e-08\n",
      "Test Loss: 6.453143441831344e-08\n",
      "\n",
      "Epoch: 712\n",
      "Training Loss: 1.7175552239715824e-08\n",
      "Test Loss: 6.387139706021117e-08\n",
      "\n",
      "Epoch: 713\n",
      "Training Loss: 1.638162139272481e-08\n",
      "Test Loss: 6.324584944650269e-08\n",
      "\n",
      "Epoch: 714\n",
      "Training Loss: 1.6779123275985814e-08\n",
      "Test Loss: 6.30067873430562e-08\n",
      "\n",
      "Epoch: 715\n",
      "Training Loss: 1.6171639434977198e-08\n",
      "Test Loss: 6.501701932393189e-08\n",
      "\n",
      "Epoch: 716\n",
      "Training Loss: 1.606887748787737e-08\n",
      "Test Loss: 6.116934514466266e-08\n",
      "\n",
      "Epoch: 717\n",
      "Training Loss: 1.5889820126346876e-08\n",
      "Test Loss: 6.097001659099988e-08\n",
      "\n",
      "Epoch: 718\n",
      "Training Loss: 1.5505025707795994e-08\n",
      "Test Loss: 6.103960714654022e-08\n",
      "\n",
      "Epoch: 719\n",
      "Training Loss: 1.5430681621353415e-08\n",
      "Test Loss: 6.067943303378343e-08\n",
      "\n",
      "Epoch: 720\n",
      "Training Loss: 1.5273117881520193e-08\n",
      "Test Loss: 6.017715747930197e-08\n",
      "\n",
      "Epoch: 721\n",
      "Training Loss: 1.625888105631172e-08\n",
      "Test Loss: 5.999831387271115e-08\n",
      "\n",
      "Epoch: 722\n",
      "Training Loss: 1.5146061590106303e-08\n",
      "Test Loss: 5.98399054752008e-08\n",
      "\n",
      "Epoch: 723\n",
      "Training Loss: 1.5143566400865893e-08\n",
      "Test Loss: 6.001096153340768e-08\n",
      "\n",
      "Epoch: 724\n",
      "Training Loss: 1.5256817735102384e-08\n",
      "Test Loss: 6.060488288994748e-08\n",
      "\n",
      "Epoch: 725\n",
      "Training Loss: 1.512942156741322e-08\n",
      "Test Loss: 6.030325039318996e-08\n",
      "\n",
      "Epoch: 726\n",
      "Training Loss: 1.4996672940507476e-08\n",
      "Test Loss: 6.041005917722941e-08\n",
      "\n",
      "Epoch: 727\n",
      "Training Loss: 1.5056536165995265e-08\n",
      "Test Loss: 5.999801544476213e-08\n",
      "\n",
      "Epoch: 728\n",
      "Training Loss: 1.4528691139048533e-08\n",
      "Test Loss: 5.991948626160593e-08\n",
      "\n",
      "Epoch: 729\n",
      "Training Loss: 1.4595582816430882e-08\n",
      "Test Loss: 5.99337681705947e-08\n",
      "\n",
      "Epoch: 730\n",
      "Training Loss: 1.453079049677323e-08\n",
      "Test Loss: 5.971610050892195e-08\n",
      "\n",
      "Epoch: 731\n",
      "Training Loss: 1.4312441010133625e-08\n",
      "Test Loss: 5.939474689853341e-08\n",
      "\n",
      "Epoch: 732\n",
      "Training Loss: 1.4468976535416308e-08\n",
      "Test Loss: 5.900317745499706e-08\n",
      "\n",
      "Epoch: 733\n",
      "Training Loss: 1.4294337269404878e-08\n",
      "Test Loss: 5.973907946099644e-08\n",
      "\n",
      "Epoch: 734\n",
      "Training Loss: 1.4444744955710576e-08\n",
      "Test Loss: 5.912611200642459e-08\n",
      "\n",
      "Epoch: 735\n",
      "Training Loss: 1.5513876405748306e-08\n",
      "Test Loss: 1.2745421429372072e-07\n",
      "\n",
      "Epoch: 736\n",
      "Training Loss: 2.1297647201379277e-08\n",
      "Test Loss: 6.469879565429437e-08\n",
      "\n",
      "Epoch: 737\n",
      "Training Loss: 1.5071340323894827e-08\n",
      "Test Loss: 5.937940628086835e-08\n",
      "\n",
      "Epoch: 738\n",
      "Training Loss: 1.5545821815029132e-08\n",
      "Test Loss: 6.215886827476425e-08\n",
      "\n",
      "Epoch: 739\n",
      "Training Loss: 1.7180435740726807e-08\n",
      "Test Loss: 6.067078572868922e-08\n",
      "\n",
      "Epoch: 740\n",
      "Training Loss: 1.5799815085889197e-08\n",
      "Test Loss: 6.352179582336248e-08\n",
      "\n",
      "Epoch: 741\n",
      "Training Loss: 1.5558931624563382e-08\n",
      "Test Loss: 6.20372588855389e-08\n",
      "\n",
      "Epoch: 742\n",
      "Training Loss: 1.7058567299462386e-08\n",
      "Test Loss: 6.644901162644601e-08\n",
      "\n",
      "Epoch: 743\n",
      "Training Loss: 1.570089924740614e-08\n",
      "Test Loss: 5.99973830617273e-08\n",
      "\n",
      "Epoch: 744\n",
      "Training Loss: 1.4789687400688459e-08\n",
      "Test Loss: 5.962520077673616e-08\n",
      "\n",
      "Epoch: 745\n",
      "Training Loss: 1.4412128083544454e-08\n",
      "Test Loss: 5.855214624261862e-08\n",
      "\n",
      "Epoch: 746\n",
      "Training Loss: 1.5299347566610777e-08\n",
      "Test Loss: 5.838854022499618e-08\n",
      "\n",
      "Epoch: 747\n",
      "Training Loss: 1.4160880468239156e-08\n",
      "Test Loss: 5.769525301957401e-08\n",
      "\n",
      "Epoch: 748\n",
      "Training Loss: 1.4976600700341198e-08\n",
      "Test Loss: 5.8679578529563514e-08\n",
      "\n",
      "Epoch: 749\n",
      "Training Loss: 1.3821278047979982e-08\n",
      "Test Loss: 5.796649915623675e-08\n",
      "\n",
      "Epoch: 750\n",
      "Training Loss: 1.3721919600584442e-08\n",
      "Test Loss: 5.741061670505587e-08\n",
      "\n",
      "Epoch: 751\n",
      "Training Loss: 1.4084385213664063e-08\n",
      "Test Loss: 5.760270838095494e-08\n",
      "\n",
      "Epoch: 752\n",
      "Training Loss: 1.3697617262664608e-08\n",
      "Test Loss: 5.799114077831291e-08\n",
      "\n",
      "Epoch: 753\n",
      "Training Loss: 1.4805490759310183e-08\n",
      "Test Loss: 5.746582587562443e-08\n",
      "\n",
      "Epoch: 754\n",
      "Training Loss: 1.4358275161422776e-08\n",
      "Test Loss: 5.7263875419266697e-08\n",
      "\n",
      "Epoch: 755\n",
      "Training Loss: 1.423151196888739e-08\n",
      "Test Loss: 5.7201532399631105e-08\n",
      "\n",
      "Epoch: 756\n",
      "Training Loss: 1.4404308856796888e-08\n",
      "Test Loss: 7.234616816731432e-08\n",
      "\n",
      "Epoch: 757\n",
      "Training Loss: 1.562987872461008e-08\n",
      "Test Loss: 5.803435243478816e-08\n",
      "\n",
      "Epoch: 758\n",
      "Training Loss: 1.4939586752878615e-08\n",
      "Test Loss: 5.70499523178114e-08\n",
      "\n",
      "Epoch: 759\n",
      "Training Loss: 1.368289037628756e-08\n",
      "Test Loss: 5.687784465635559e-08\n",
      "\n",
      "Epoch: 760\n",
      "Training Loss: 1.3703249202023926e-08\n",
      "Test Loss: 5.651786949556481e-08\n",
      "\n",
      "Epoch: 761\n",
      "Training Loss: 1.3555224498418283e-08\n",
      "Test Loss: 5.66379618760493e-08\n",
      "\n",
      "Epoch: 762\n",
      "Training Loss: 1.3471595394776159e-08\n",
      "Test Loss: 5.577065920192581e-08\n",
      "\n",
      "Epoch: 763\n",
      "Training Loss: 1.3421266172504906e-08\n",
      "Test Loss: 5.618378295935145e-08\n",
      "\n",
      "Epoch: 764\n",
      "Training Loss: 1.3355000364848971e-08\n",
      "Test Loss: 5.6146674864976376e-08\n",
      "\n",
      "Epoch: 765\n",
      "Training Loss: 1.3280696246435278e-08\n",
      "Test Loss: 5.591051532860547e-08\n",
      "\n",
      "Epoch: 766\n",
      "Training Loss: 1.3361463639209129e-08\n",
      "Test Loss: 5.588740492612487e-08\n",
      "\n",
      "Epoch: 767\n",
      "Training Loss: 1.3347169887841423e-08\n",
      "Test Loss: 5.580356443601886e-08\n",
      "\n",
      "Epoch: 768\n",
      "Training Loss: 1.4081853905167918e-08\n",
      "Test Loss: 5.7138507258969184e-08\n",
      "\n",
      "Epoch: 769\n",
      "Training Loss: 1.565088147970073e-08\n",
      "Test Loss: 5.645015477284687e-08\n",
      "\n",
      "Epoch: 770\n",
      "Training Loss: 1.3593087248390626e-08\n",
      "Test Loss: 5.6715496299375445e-08\n",
      "\n",
      "Epoch: 771\n",
      "Training Loss: 1.357744938701444e-08\n",
      "Test Loss: 5.644000466986654e-08\n",
      "\n",
      "Epoch: 772\n",
      "Training Loss: 1.3282345593760661e-08\n",
      "Test Loss: 5.617902587573553e-08\n",
      "\n",
      "Epoch: 773\n",
      "Training Loss: 1.6852506649437753e-08\n",
      "Test Loss: 5.6884147170421784e-08\n",
      "\n",
      "Epoch: 774\n",
      "Training Loss: 1.3401708187643635e-08\n",
      "Test Loss: 5.568388061760743e-08\n",
      "\n",
      "Epoch: 775\n",
      "Training Loss: 1.3289850997466601e-08\n",
      "Test Loss: 5.544639947174801e-08\n",
      "\n",
      "Epoch: 776\n",
      "Training Loss: 1.325414652105413e-08\n",
      "Test Loss: 5.545120274064175e-08\n",
      "\n",
      "Epoch: 777\n",
      "Training Loss: 1.3106890091781528e-08\n",
      "Test Loss: 5.544801950918554e-08\n",
      "\n",
      "Epoch: 778\n",
      "Training Loss: 1.3081394634184562e-08\n",
      "Test Loss: 5.52792691621562e-08\n",
      "\n",
      "Epoch: 779\n",
      "Training Loss: 1.3471352922067581e-08\n",
      "Test Loss: 5.545971859532983e-08\n",
      "\n",
      "Epoch: 780\n",
      "Training Loss: 1.4731371974126736e-08\n",
      "Test Loss: 5.473578923442801e-08\n",
      "\n",
      "Epoch: 781\n",
      "Training Loss: 1.4708875006874678e-08\n",
      "Test Loss: 5.3844679825942876e-08\n",
      "\n",
      "Epoch: 782\n",
      "Training Loss: 1.560198755375571e-08\n",
      "Test Loss: 5.408057290878787e-08\n",
      "\n",
      "Epoch: 783\n",
      "Training Loss: 1.78315667416958e-08\n",
      "Test Loss: 5.905589972599046e-08\n",
      "\n",
      "Epoch: 784\n",
      "Training Loss: 1.7388766574792196e-08\n",
      "Test Loss: 5.8138098779636493e-08\n",
      "\n",
      "Epoch: 785\n",
      "Training Loss: 1.6886270752062654e-08\n",
      "Test Loss: 6.758737214340726e-08\n",
      "\n",
      "Epoch: 786\n",
      "Training Loss: 1.8632424492182054e-08\n",
      "Test Loss: 5.959447335612822e-08\n",
      "\n",
      "Epoch: 787\n",
      "Training Loss: 1.6780081028381726e-08\n",
      "Test Loss: 5.740037778423357e-08\n",
      "\n",
      "Epoch: 788\n",
      "Training Loss: 1.632886773942725e-08\n",
      "Test Loss: 5.732151109327788e-08\n",
      "\n",
      "Epoch: 789\n",
      "Training Loss: 1.5909358867342387e-08\n",
      "Test Loss: 5.582485940180959e-08\n",
      "\n",
      "Epoch: 790\n",
      "Training Loss: 1.7976937864242853e-08\n",
      "Test Loss: 6.1932368566886e-08\n",
      "\n",
      "Epoch: 791\n",
      "Training Loss: 1.6917897305290808e-08\n",
      "Test Loss: 5.4892840495313067e-08\n",
      "\n",
      "Epoch: 792\n",
      "Training Loss: 1.5521351611387974e-08\n",
      "Test Loss: 5.425387072932608e-08\n",
      "\n",
      "Epoch: 793\n",
      "Training Loss: 1.4608356006344062e-08\n",
      "Test Loss: 5.3671591615511716e-08\n",
      "\n",
      "Epoch: 794\n",
      "Training Loss: 1.4547264726161302e-08\n",
      "Test Loss: 5.439908434823337e-08\n",
      "\n",
      "Epoch: 795\n",
      "Training Loss: 1.4366204226234913e-08\n",
      "Test Loss: 5.381289369665865e-08\n",
      "\n",
      "Epoch: 796\n",
      "Training Loss: 1.425361636127794e-08\n",
      "Test Loss: 5.310503681243972e-08\n",
      "\n",
      "Epoch: 797\n",
      "Training Loss: 1.4607991261073039e-08\n",
      "Test Loss: 5.296832839007948e-08\n",
      "\n",
      "Epoch: 798\n",
      "Training Loss: 1.525703918758836e-08\n",
      "Test Loss: 5.284757165213705e-08\n",
      "\n",
      "Epoch: 799\n",
      "Training Loss: 1.4506658689109978e-08\n",
      "Test Loss: 5.2466123889871596e-08\n",
      "\n",
      "Epoch: 800\n",
      "Training Loss: 1.4241436770608592e-08\n",
      "Test Loss: 5.2147907325661436e-08\n",
      "\n",
      "Epoch: 801\n",
      "Training Loss: 1.4085882682479678e-08\n",
      "Test Loss: 5.2233598779594104e-08\n",
      "\n",
      "Epoch: 802\n",
      "Training Loss: 1.4046180811059609e-08\n",
      "Test Loss: 5.2046022602780795e-08\n",
      "\n",
      "Epoch: 803\n",
      "Training Loss: 1.3896946112386862e-08\n",
      "Test Loss: 5.213352238797597e-08\n",
      "\n",
      "Epoch: 804\n",
      "Training Loss: 1.40740684292003e-08\n",
      "Test Loss: 5.211623843592861e-08\n",
      "\n",
      "Epoch: 805\n",
      "Training Loss: 1.4046175185929618e-08\n",
      "Test Loss: 5.767452648797189e-08\n",
      "\n",
      "Epoch: 806\n",
      "Training Loss: 1.4458926796597401e-08\n",
      "Test Loss: 5.1375490528471346e-08\n",
      "\n",
      "Epoch: 807\n",
      "Training Loss: 1.4284559905301345e-08\n",
      "Test Loss: 5.1300265369036424e-08\n",
      "\n",
      "Epoch: 808\n",
      "Training Loss: 1.3869535149998077e-08\n",
      "Test Loss: 5.307995465386739e-08\n",
      "\n",
      "Epoch: 809\n",
      "Training Loss: 1.3907659024425811e-08\n",
      "Test Loss: 5.101544076069331e-08\n",
      "\n",
      "Epoch: 810\n",
      "Training Loss: 1.3728795581850287e-08\n",
      "Test Loss: 5.090796051376856e-08\n",
      "\n",
      "Epoch: 811\n",
      "Training Loss: 1.3843221384017093e-08\n",
      "Test Loss: 5.095022004297789e-08\n",
      "\n",
      "Epoch: 812\n",
      "Training Loss: 1.3750914481155027e-08\n",
      "Test Loss: 5.082763365749088e-08\n",
      "\n",
      "Epoch: 813\n",
      "Training Loss: 1.445977648728558e-08\n",
      "Test Loss: 5.0829537912022715e-08\n",
      "\n",
      "Epoch: 814\n",
      "Training Loss: 1.286049163458832e-08\n",
      "Test Loss: 5.062969421487651e-08\n",
      "\n",
      "Epoch: 815\n",
      "Training Loss: 1.2762217653043232e-08\n",
      "Test Loss: 5.3041940617504224e-08\n",
      "\n",
      "Epoch: 816\n",
      "Training Loss: 1.2893004885938808e-08\n",
      "Test Loss: 5.065876607091013e-08\n",
      "\n",
      "Epoch: 817\n",
      "Training Loss: 1.2775819809481467e-08\n",
      "Test Loss: 5.027630578524622e-08\n",
      "\n",
      "Epoch: 818\n",
      "Training Loss: 1.2537315186023079e-08\n",
      "Test Loss: 5.014934600922061e-08\n",
      "\n",
      "Epoch: 819\n",
      "Training Loss: 1.2937196499270462e-08\n",
      "Test Loss: 5.028190130929033e-08\n",
      "\n",
      "Epoch: 820\n",
      "Training Loss: 1.2669597815317957e-08\n",
      "Test Loss: 5.0226891090687786e-08\n",
      "\n",
      "Epoch: 821\n",
      "Training Loss: 1.2517957633425189e-08\n",
      "Test Loss: 7.991677364316274e-08\n",
      "\n",
      "Epoch: 822\n",
      "Training Loss: 2.1016609193698816e-08\n",
      "Test Loss: 5.5536425236368814e-08\n",
      "\n",
      "Epoch: 823\n",
      "Training Loss: 1.3038684502456968e-08\n",
      "Test Loss: 5.087933985237214e-08\n",
      "\n",
      "Epoch: 824\n",
      "Training Loss: 1.29911228441415e-08\n",
      "Test Loss: 5.0944162666155535e-08\n",
      "\n",
      "Epoch: 825\n",
      "Training Loss: 1.2428808500904628e-08\n",
      "Test Loss: 5.107026979089824e-08\n",
      "\n",
      "Epoch: 826\n",
      "Training Loss: 1.2200218317085879e-08\n",
      "Test Loss: 5.1937764311560386e-08\n",
      "\n",
      "Epoch: 827\n",
      "Training Loss: 1.2083164098915708e-08\n",
      "Test Loss: 4.991968793888191e-08\n",
      "\n",
      "Epoch: 828\n",
      "Training Loss: 1.2068179640796947e-08\n",
      "Test Loss: 4.976137901735456e-08\n",
      "\n",
      "Epoch: 829\n",
      "Training Loss: 1.3705111712170037e-08\n",
      "Test Loss: 4.9577426608493624e-08\n",
      "\n",
      "Epoch: 830\n",
      "Training Loss: 1.1652616057252393e-08\n",
      "Test Loss: 4.944566356357427e-08\n",
      "\n",
      "Epoch: 831\n",
      "Training Loss: 1.185645744546567e-08\n",
      "Test Loss: 4.86415530076556e-08\n",
      "\n",
      "Epoch: 832\n",
      "Training Loss: 1.1809046481422078e-08\n",
      "Test Loss: 4.853371393664929e-08\n",
      "\n",
      "Epoch: 833\n",
      "Training Loss: 1.1878285910427167e-08\n",
      "Test Loss: 5.705946648504323e-08\n",
      "\n",
      "Epoch: 834\n",
      "Training Loss: 1.2990968005036999e-08\n",
      "Test Loss: 4.928119778924156e-08\n",
      "\n",
      "Epoch: 835\n",
      "Training Loss: 1.239281862315996e-08\n",
      "Test Loss: 4.8702915478315845e-08\n",
      "\n",
      "Epoch: 836\n",
      "Training Loss: 1.1649620527502217e-08\n",
      "Test Loss: 4.8556628939877555e-08\n",
      "\n",
      "Epoch: 837\n",
      "Training Loss: 1.1657463439007643e-08\n",
      "Test Loss: 4.934149444579816e-08\n",
      "\n",
      "Epoch: 838\n",
      "Training Loss: 1.215785635129881e-08\n",
      "Test Loss: 4.8392220008963704e-08\n",
      "\n",
      "Epoch: 839\n",
      "Training Loss: 1.217710673036739e-08\n",
      "Test Loss: 4.8559801513192724e-08\n",
      "\n",
      "Epoch: 840\n",
      "Training Loss: 1.1862401431509776e-08\n",
      "Test Loss: 4.866984326667989e-08\n",
      "\n",
      "Epoch: 841\n",
      "Training Loss: 1.2770885978360033e-08\n",
      "Test Loss: 4.875660408742988e-08\n",
      "\n",
      "Epoch: 842\n",
      "Training Loss: 1.1719841689720548e-08\n",
      "Test Loss: 4.841901457552922e-08\n",
      "\n",
      "Epoch: 843\n",
      "Training Loss: 1.2442706420756622e-08\n",
      "Test Loss: 7.500367615875803e-08\n",
      "\n",
      "Epoch: 844\n",
      "Training Loss: 1.348291907750839e-08\n",
      "Test Loss: 4.875798254033725e-08\n",
      "\n",
      "Epoch: 845\n",
      "Training Loss: 1.1579915396945731e-08\n",
      "Test Loss: 4.789561458551361e-08\n",
      "\n",
      "Epoch: 846\n",
      "Training Loss: 1.169193423559515e-08\n",
      "Test Loss: 4.788713070524864e-08\n",
      "\n",
      "Epoch: 847\n",
      "Training Loss: 1.1456911567601461e-08\n",
      "Test Loss: 5.3974829938852054e-08\n",
      "\n",
      "Epoch: 848\n",
      "Training Loss: 1.2797652419275588e-08\n",
      "Test Loss: 4.791846208718198e-08\n",
      "\n",
      "Epoch: 849\n",
      "Training Loss: 1.1532343080489227e-08\n",
      "Test Loss: 4.737546532851411e-08\n",
      "\n",
      "Epoch: 850\n",
      "Training Loss: 1.1365294187252554e-08\n",
      "Test Loss: 4.743669279605456e-08\n",
      "\n",
      "Epoch: 851\n",
      "Training Loss: 1.3005882297060603e-08\n",
      "Test Loss: 5.61966615464371e-08\n",
      "\n",
      "Epoch: 852\n",
      "Training Loss: 2.3580701835612672e-08\n",
      "Test Loss: 6.883431069582002e-08\n",
      "\n",
      "Epoch: 853\n",
      "Training Loss: 1.992090166898682e-08\n",
      "Test Loss: 5.408119463368166e-08\n",
      "\n",
      "Epoch: 854\n",
      "Training Loss: 1.6055169342147717e-08\n",
      "Test Loss: 5.4648971570259164e-08\n",
      "\n",
      "Epoch: 855\n",
      "Training Loss: 1.5407884153736024e-08\n",
      "Test Loss: 5.643251554943163e-08\n",
      "\n",
      "Epoch: 856\n",
      "Training Loss: 1.4664090090358664e-08\n",
      "Test Loss: 5.602593944331602e-08\n",
      "\n",
      "Epoch: 857\n",
      "Training Loss: 1.4076876257244445e-08\n",
      "Test Loss: 5.350378629032093e-08\n",
      "\n",
      "Epoch: 858\n",
      "Training Loss: 1.3539619795703098e-08\n",
      "Test Loss: 5.03347798996856e-08\n",
      "\n",
      "Epoch: 859\n",
      "Training Loss: 1.281568332937392e-08\n",
      "Test Loss: 4.901703221094067e-08\n",
      "\n",
      "Epoch: 860\n",
      "Training Loss: 1.2648948851297822e-08\n",
      "Test Loss: 4.78423913818915e-08\n",
      "\n",
      "Epoch: 861\n",
      "Training Loss: 1.1631115034068292e-08\n",
      "Test Loss: 4.6784155216528234e-08\n",
      "\n",
      "Epoch: 862\n",
      "Training Loss: 1.1781124520382491e-08\n",
      "Test Loss: 4.6288072041988926e-08\n",
      "\n",
      "Epoch: 863\n",
      "Training Loss: 1.130475653828474e-08\n",
      "Test Loss: 5.889354781629663e-08\n",
      "\n",
      "Epoch: 864\n",
      "Training Loss: 1.379571508882312e-08\n",
      "Test Loss: 4.8444608324871297e-08\n",
      "\n",
      "Epoch: 865\n",
      "Training Loss: 1.170715198857882e-08\n",
      "Test Loss: 4.6278771037577826e-08\n",
      "\n",
      "Epoch: 866\n",
      "Training Loss: 1.1106519851959243e-08\n",
      "Test Loss: 4.677753295823095e-08\n",
      "\n",
      "Epoch: 867\n",
      "Training Loss: 1.1155090777018964e-08\n",
      "Test Loss: 4.692346777801504e-08\n",
      "\n",
      "Epoch: 868\n",
      "Training Loss: 1.1070826921866228e-08\n",
      "Test Loss: 4.81768580584685e-08\n",
      "\n",
      "Epoch: 869\n",
      "Training Loss: 1.1090810936309481e-08\n",
      "Test Loss: 4.6535678421832927e-08\n",
      "\n",
      "Epoch: 870\n",
      "Training Loss: 1.1113875449571728e-08\n",
      "Test Loss: 4.638612338681014e-08\n",
      "\n",
      "Epoch: 871\n",
      "Training Loss: 1.1835061227335094e-08\n",
      "Test Loss: 4.6107434314990314e-08\n",
      "\n",
      "Epoch: 872\n",
      "Training Loss: 1.1044136864294766e-08\n",
      "Test Loss: 4.5701902706696274e-08\n",
      "\n",
      "Epoch: 873\n",
      "Training Loss: 1.0933839019363253e-08\n",
      "Test Loss: 4.533078978852245e-08\n",
      "\n",
      "Epoch: 874\n",
      "Training Loss: 1.0854511067748263e-08\n",
      "Test Loss: 4.536705588975565e-08\n",
      "\n",
      "Epoch: 875\n",
      "Training Loss: 1.088462712554398e-08\n",
      "Test Loss: 5.5186248459904164e-08\n",
      "\n",
      "Epoch: 876\n",
      "Training Loss: 1.3017974254125875e-08\n",
      "Test Loss: 4.533840325393612e-08\n",
      "\n",
      "Epoch: 877\n",
      "Training Loss: 1.0894972923836121e-08\n",
      "Test Loss: 4.481887927454409e-08\n",
      "\n",
      "Epoch: 878\n",
      "Training Loss: 1.0904994240945598e-08\n",
      "Test Loss: 4.53755433227343e-08\n",
      "\n",
      "Epoch: 879\n",
      "Training Loss: 1.0815952874073295e-08\n",
      "Test Loss: 4.5204728849057574e-08\n",
      "\n",
      "Epoch: 880\n",
      "Training Loss: 1.0762206237302507e-08\n",
      "Test Loss: 4.4755953609865173e-08\n",
      "\n",
      "Epoch: 881\n",
      "Training Loss: 1.2169269740051428e-08\n",
      "Test Loss: 4.434149403209631e-08\n",
      "\n",
      "Epoch: 882\n",
      "Training Loss: 1.1181320166050076e-08\n",
      "Test Loss: 4.474397030662658e-08\n",
      "\n",
      "Epoch: 883\n",
      "Training Loss: 1.0807477283473569e-08\n",
      "Test Loss: 4.472534698152231e-08\n",
      "\n",
      "Epoch: 884\n",
      "Training Loss: 1.0769576933948125e-08\n",
      "Test Loss: 4.433497480249571e-08\n",
      "\n",
      "Epoch: 885\n",
      "Training Loss: 1.0736913580444707e-08\n",
      "Test Loss: 4.4212981720193056e-08\n",
      "\n",
      "Epoch: 886\n",
      "Training Loss: 1.072632086855189e-08\n",
      "Test Loss: 4.864596903075835e-08\n",
      "\n",
      "Epoch: 887\n",
      "Training Loss: 1.134315012289259e-08\n",
      "Test Loss: 4.442782142177748e-08\n",
      "\n",
      "Epoch: 888\n",
      "Training Loss: 1.0685038335604228e-08\n",
      "Test Loss: 4.4316379899100866e-08\n",
      "\n",
      "Epoch: 889\n",
      "Training Loss: 1.1121723986207144e-08\n",
      "Test Loss: 4.456848401446223e-08\n",
      "\n",
      "Epoch: 890\n",
      "Training Loss: 1.0608899536634908e-08\n",
      "Test Loss: 4.446522794410157e-08\n",
      "\n",
      "Epoch: 891\n",
      "Training Loss: 1.211603943100196e-08\n",
      "Test Loss: 1.020010387264847e-07\n",
      "\n",
      "Epoch: 892\n",
      "Training Loss: 2.1294409198920523e-08\n",
      "Test Loss: 4.7469811192968336e-08\n",
      "\n",
      "Epoch: 893\n",
      "Training Loss: 1.1611516785118662e-08\n",
      "Test Loss: 4.571927192387193e-08\n",
      "\n",
      "Epoch: 894\n",
      "Training Loss: 1.1978690928060587e-08\n",
      "Test Loss: 4.626635430327042e-08\n",
      "\n",
      "Epoch: 895\n",
      "Training Loss: 1.1216862105811742e-08\n",
      "Test Loss: 4.5094218137364805e-08\n",
      "\n",
      "Epoch: 896\n",
      "Training Loss: 1.0771732542972737e-08\n",
      "Test Loss: 4.5283183425226525e-08\n",
      "\n",
      "Epoch: 897\n",
      "Training Loss: 1.058587055050945e-08\n",
      "Test Loss: 4.484268245619205e-08\n",
      "\n",
      "Epoch: 898\n",
      "Training Loss: 1.0481712090874376e-08\n",
      "Test Loss: 4.450711799108831e-08\n",
      "\n",
      "Epoch: 899\n",
      "Training Loss: 1.0392199693380158e-08\n",
      "Test Loss: 4.376795814664547e-08\n",
      "\n",
      "Epoch: 900\n",
      "Training Loss: 1.0304092394145906e-08\n",
      "Test Loss: 4.3796106297122606e-08\n",
      "\n",
      "Epoch: 901\n",
      "Training Loss: 1.0088125594146883e-08\n",
      "Test Loss: 4.3757786727383063e-08\n",
      "\n",
      "Epoch: 902\n",
      "Training Loss: 1.0370918938444143e-08\n",
      "Test Loss: 4.3826286599824016e-08\n",
      "\n",
      "Epoch: 903\n",
      "Training Loss: 1.0208682195885407e-08\n",
      "Test Loss: 4.3856797304897555e-08\n",
      "\n",
      "Epoch: 904\n",
      "Training Loss: 1.0161443242395762e-08\n",
      "Test Loss: 4.379235463147779e-08\n",
      "\n",
      "Epoch: 905\n",
      "Training Loss: 1.005744406275729e-08\n",
      "Test Loss: 4.366446049175465e-08\n",
      "\n",
      "Epoch: 906\n",
      "Training Loss: 1.008510815599569e-08\n",
      "Test Loss: 4.3446490849419206e-08\n",
      "\n",
      "Epoch: 907\n",
      "Training Loss: 1.011924410931897e-08\n",
      "Test Loss: 5.413635051354504e-08\n",
      "\n",
      "Epoch: 908\n",
      "Training Loss: 1.1477745272732895e-08\n",
      "Test Loss: 4.4074841554220257e-08\n",
      "\n",
      "Epoch: 909\n",
      "Training Loss: 1.0126623687748785e-08\n",
      "Test Loss: 4.312799362082842e-08\n",
      "\n",
      "Epoch: 910\n",
      "Training Loss: 1.0190677339020718e-08\n",
      "Test Loss: 4.3570331342834834e-08\n",
      "\n",
      "Epoch: 911\n",
      "Training Loss: 9.899669972668335e-09\n",
      "Test Loss: 6.118419548784004e-08\n",
      "\n",
      "Epoch: 912\n",
      "Training Loss: 1.0837087967748479e-08\n",
      "Test Loss: 4.280580512272536e-08\n",
      "\n",
      "Epoch: 913\n",
      "Training Loss: 1.0441837912840887e-08\n",
      "Test Loss: 4.2919111820083344e-08\n",
      "\n",
      "Epoch: 914\n",
      "Training Loss: 9.855686933235575e-09\n",
      "Test Loss: 4.342022918990551e-08\n",
      "\n",
      "Epoch: 915\n",
      "Training Loss: 9.901858888383686e-09\n",
      "Test Loss: 4.339032955158473e-08\n",
      "\n",
      "Epoch: 916\n",
      "Training Loss: 1.0301090055027847e-08\n",
      "Test Loss: 5.225658838980962e-08\n",
      "\n",
      "Epoch: 917\n",
      "Training Loss: 1.1065318439307248e-08\n",
      "Test Loss: 4.3192578402795334e-08\n",
      "\n",
      "Epoch: 918\n",
      "Training Loss: 1.0111753804646165e-08\n",
      "Test Loss: 4.321374547089363e-08\n",
      "\n",
      "Epoch: 919\n",
      "Training Loss: 9.990843115777656e-09\n",
      "Test Loss: 5.152926263463087e-08\n",
      "\n",
      "Epoch: 920\n",
      "Training Loss: 1.3402689032678458e-08\n",
      "Test Loss: 4.3931166260335885e-08\n",
      "\n",
      "Epoch: 921\n",
      "Training Loss: 1.0072616222582079e-08\n",
      "Test Loss: 4.278814813574172e-08\n",
      "\n",
      "Epoch: 922\n",
      "Training Loss: 1.0437723278281888e-08\n",
      "Test Loss: 4.309164225446693e-08\n",
      "\n",
      "Epoch: 923\n",
      "Training Loss: 9.779435187577215e-09\n",
      "Test Loss: 4.279430854126076e-08\n",
      "\n",
      "Epoch: 924\n",
      "Training Loss: 9.98607034100966e-09\n",
      "Test Loss: 4.52496031755345e-08\n",
      "\n",
      "Epoch: 925\n",
      "Training Loss: 9.577343954939957e-09\n",
      "Test Loss: 4.208229498203764e-08\n",
      "\n",
      "Epoch: 926\n",
      "Training Loss: 9.939722526534448e-09\n",
      "Test Loss: 4.224578376010868e-08\n",
      "\n",
      "Epoch: 927\n",
      "Training Loss: 9.730941386010272e-09\n",
      "Test Loss: 4.2260843713393115e-08\n",
      "\n",
      "Epoch: 928\n",
      "Training Loss: 9.682472305409343e-09\n",
      "Test Loss: 4.181411838999338e-08\n",
      "\n",
      "Epoch: 929\n",
      "Training Loss: 1.0469143774116674e-08\n",
      "Test Loss: 4.137503495371675e-08\n",
      "\n",
      "Epoch: 930\n",
      "Training Loss: 9.622603750851036e-09\n",
      "Test Loss: 4.1627366442753555e-08\n",
      "\n",
      "Epoch: 931\n",
      "Training Loss: 9.641103915214444e-09\n",
      "Test Loss: 4.205604753337866e-08\n",
      "\n",
      "Epoch: 932\n",
      "Training Loss: 9.599821382266782e-09\n",
      "Test Loss: 4.170594891661494e-08\n",
      "\n",
      "Epoch: 933\n",
      "Training Loss: 9.895039602506964e-09\n",
      "Test Loss: 4.208519754911322e-08\n",
      "\n",
      "Epoch: 934\n",
      "Training Loss: 9.631453856684402e-09\n",
      "Test Loss: 4.2041072845222516e-08\n",
      "\n",
      "Epoch: 935\n",
      "Training Loss: 1.0009385912705207e-08\n",
      "Test Loss: 4.1952876728146293e-08\n",
      "\n",
      "Epoch: 936\n",
      "Training Loss: 9.517441057482756e-09\n",
      "Test Loss: 4.1490554991696627e-08\n",
      "\n",
      "Epoch: 937\n",
      "Training Loss: 9.589186185839557e-09\n",
      "Test Loss: 4.601512060276036e-08\n",
      "\n",
      "Epoch: 938\n",
      "Training Loss: 1.1356789582824453e-08\n",
      "Test Loss: 4.1458246613501615e-08\n",
      "\n",
      "Epoch: 939\n",
      "Training Loss: 9.620498175877401e-09\n",
      "Test Loss: 4.124201069544142e-08\n",
      "\n",
      "Epoch: 940\n",
      "Training Loss: 9.569553297931558e-09\n",
      "Test Loss: 4.171330658664374e-08\n",
      "\n",
      "Epoch: 941\n",
      "Training Loss: 9.551395822408418e-09\n",
      "Test Loss: 4.1706684328346455e-08\n",
      "\n",
      "Epoch: 942\n",
      "Training Loss: 9.461453842618539e-09\n",
      "Test Loss: 4.1333194644721516e-08\n",
      "\n",
      "Epoch: 943\n",
      "Training Loss: 1.310116578186656e-08\n",
      "Test Loss: 4.529205455128249e-08\n",
      "\n",
      "Epoch: 944\n",
      "Training Loss: 1.0031074933654812e-08\n",
      "Test Loss: 4.244737183967118e-08\n",
      "\n",
      "Epoch: 945\n",
      "Training Loss: 9.830870340010733e-09\n",
      "Test Loss: 4.2969521274471845e-08\n",
      "\n",
      "Epoch: 946\n",
      "Training Loss: 9.722689320312838e-09\n",
      "Test Loss: 4.279891285818849e-08\n",
      "\n",
      "Epoch: 947\n",
      "Training Loss: 9.735338757366208e-09\n",
      "Test Loss: 4.291227995167901e-08\n",
      "\n",
      "Epoch: 948\n",
      "Training Loss: 9.479660612043972e-09\n",
      "Test Loss: 4.110775364551955e-08\n",
      "\n",
      "Epoch: 949\n",
      "Training Loss: 9.612590131287865e-09\n",
      "Test Loss: 4.095371153312044e-08\n",
      "\n",
      "Epoch: 950\n",
      "Training Loss: 9.98030458276844e-09\n",
      "Test Loss: 4.288358113058166e-08\n",
      "\n",
      "Epoch: 951\n",
      "Training Loss: 9.4061238797849e-09\n",
      "Test Loss: 4.034073697312124e-08\n",
      "\n",
      "Epoch: 952\n",
      "Training Loss: 9.316657371509033e-09\n",
      "Test Loss: 4.033043410345272e-08\n",
      "\n",
      "Epoch: 953\n",
      "Training Loss: 9.313092371362094e-09\n",
      "Test Loss: 4.079339532836457e-08\n",
      "\n",
      "Epoch: 954\n",
      "Training Loss: 1.1240033200484353e-08\n",
      "Test Loss: 4.453414348404294e-08\n",
      "\n",
      "Epoch: 955\n",
      "Training Loss: 1.1768410542363958e-08\n",
      "Test Loss: 4.0857202066035825e-08\n",
      "\n",
      "Epoch: 956\n",
      "Training Loss: 1.0305276928098314e-08\n",
      "Test Loss: 4.324472513417277e-08\n",
      "\n",
      "Epoch: 957\n",
      "Training Loss: 1.0522146413431225e-08\n",
      "Test Loss: 4.110624729491974e-08\n",
      "\n",
      "Epoch: 958\n",
      "Training Loss: 9.92555608074023e-09\n",
      "Test Loss: 5.053864171600253e-08\n",
      "\n",
      "Epoch: 959\n",
      "Training Loss: 1.0654882309779623e-08\n",
      "Test Loss: 3.9740832846746343e-08\n",
      "\n",
      "Epoch: 960\n",
      "Training Loss: 9.76533254259948e-09\n",
      "Test Loss: 4.3218108203291195e-08\n",
      "\n",
      "Epoch: 961\n",
      "Training Loss: 1.029655886479001e-08\n",
      "Test Loss: 4.2077619610836337e-08\n",
      "\n",
      "Epoch: 962\n",
      "Training Loss: 1.0748498979751275e-08\n",
      "Test Loss: 4.0421397784484725e-08\n",
      "\n",
      "Epoch: 963\n",
      "Training Loss: 9.898122913890953e-09\n",
      "Test Loss: 4.084753157940213e-08\n",
      "\n",
      "Epoch: 964\n",
      "Training Loss: 9.830266378685337e-09\n",
      "Test Loss: 4.361568883837208e-08\n",
      "\n",
      "Epoch: 965\n",
      "Training Loss: 1.1096521923548153e-08\n",
      "Test Loss: 4.7807080960637904e-08\n",
      "\n",
      "Epoch: 966\n",
      "Training Loss: 3.003641246834832e-08\n",
      "Test Loss: 5.167186145627056e-08\n",
      "\n",
      "Epoch: 967\n",
      "Training Loss: 1.7060568661501445e-08\n",
      "Test Loss: 5.313883377766615e-08\n",
      "\n",
      "Epoch: 968\n",
      "Training Loss: 2.092077266979686e-08\n",
      "Test Loss: 4.4647187280588696e-08\n",
      "\n",
      "Epoch: 969\n",
      "Training Loss: 1.5645754321743272e-08\n",
      "Test Loss: 4.896345373595068e-08\n",
      "\n",
      "Epoch: 970\n",
      "Training Loss: 1.439833822540019e-08\n",
      "Test Loss: 4.522271623841334e-08\n",
      "\n",
      "Epoch: 971\n",
      "Training Loss: 1.4366078104899316e-08\n",
      "Test Loss: 4.24833821455195e-08\n",
      "\n",
      "Epoch: 972\n",
      "Training Loss: 1.2019612564510377e-08\n",
      "Test Loss: 4.3404821070680555e-08\n",
      "\n",
      "Epoch: 973\n",
      "Training Loss: 1.151068040883274e-08\n",
      "Test Loss: 4.1917889603837466e-08\n",
      "\n",
      "Epoch: 974\n",
      "Training Loss: 1.1766217333786244e-08\n",
      "Test Loss: 4.04328481806715e-08\n",
      "\n",
      "Epoch: 975\n",
      "Training Loss: 1.1199116597045608e-08\n",
      "Test Loss: 4.081697113633709e-08\n",
      "\n",
      "Epoch: 976\n",
      "Training Loss: 1.0944308570515204e-08\n",
      "Test Loss: 4.0323453021073874e-08\n",
      "\n",
      "Epoch: 977\n",
      "Training Loss: 1.1672511549913148e-08\n",
      "Test Loss: 4.0554979818807624e-08\n",
      "\n",
      "Epoch: 978\n",
      "Training Loss: 1.0733058293984262e-08\n",
      "Test Loss: 4.090484040375486e-08\n",
      "\n",
      "Epoch: 979\n",
      "Training Loss: 1.1015163596065728e-08\n",
      "Test Loss: 4.1152699026270056e-08\n",
      "\n",
      "Epoch: 980\n",
      "Training Loss: 1.1086239481983284e-08\n",
      "Test Loss: 4.136617448580182e-08\n",
      "\n",
      "Epoch: 981\n",
      "Training Loss: 1.0858617116582536e-08\n",
      "Test Loss: 4.1445982645882395e-08\n",
      "\n",
      "Epoch: 982\n",
      "Training Loss: 1.080270036387295e-08\n",
      "Test Loss: 4.0834777337295236e-08\n",
      "\n",
      "Epoch: 983\n",
      "Training Loss: 1.0757521836287273e-08\n",
      "Test Loss: 4.0426343161925615e-08\n",
      "\n",
      "Epoch: 984\n",
      "Training Loss: 1.091003406135845e-08\n",
      "Test Loss: 4.027518230032001e-08\n",
      "\n",
      "Epoch: 985\n",
      "Training Loss: 1.04845444918548e-08\n",
      "Test Loss: 3.983560148412835e-08\n",
      "\n",
      "Epoch: 986\n",
      "Training Loss: 1.0378463125941076e-08\n",
      "Test Loss: 3.9345422919723205e-08\n",
      "\n",
      "Epoch: 987\n",
      "Training Loss: 1.0314142429024287e-08\n",
      "Test Loss: 3.943095450154033e-08\n",
      "\n",
      "Epoch: 988\n",
      "Training Loss: 1.0387902390126177e-08\n",
      "Test Loss: 3.93836998568986e-08\n",
      "\n",
      "Epoch: 989\n",
      "Training Loss: 1.0560237721316904e-08\n",
      "Test Loss: 4.2453091708694046e-08\n",
      "\n",
      "Epoch: 990\n",
      "Training Loss: 1.5757104065983185e-08\n",
      "Test Loss: 5.5007792099104336e-08\n",
      "\n",
      "Epoch: 991\n",
      "Training Loss: 1.5250467851520472e-08\n",
      "Test Loss: 5.6430568662335645e-08\n",
      "\n",
      "Epoch: 992\n",
      "Training Loss: 1.426382212343924e-08\n",
      "Test Loss: 4.558880561944534e-08\n",
      "\n",
      "Epoch: 993\n",
      "Training Loss: 1.4059600002743386e-08\n",
      "Test Loss: 5.0074746127393155e-08\n",
      "\n",
      "Epoch: 994\n",
      "Training Loss: 1.3497441090729959e-08\n",
      "Test Loss: 4.482733828581331e-08\n",
      "\n",
      "Epoch: 995\n",
      "Training Loss: 1.3550918017320631e-08\n",
      "Test Loss: 4.156222033202539e-08\n",
      "\n",
      "Epoch: 996\n",
      "Training Loss: 1.0806908553225488e-08\n",
      "Test Loss: 3.975592477445389e-08\n",
      "\n",
      "Epoch: 997\n",
      "Training Loss: 9.969169193861186e-09\n",
      "Test Loss: 4.014201948621121e-08\n",
      "\n",
      "Epoch: 998\n",
      "Training Loss: 1.0646604782967492e-08\n",
      "Test Loss: 5.0048207356212515e-08\n",
      "\n",
      "Epoch: 999\n",
      "Training Loss: 1.0741755337069966e-08\n",
      "Test Loss: 3.936051840014443e-08\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_cp(model, optimizer, device, train_set, test_set, num_epoch, args.model_dir, args.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b1106-7f2f-4e83-9936-ea16f99a1d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f8575-c28a-4bdf-8342-c96e8f0d8eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e775ad39-a447-4aee-9d94-2619c598f3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l =  torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d493ec09-c2eb-4656-8e0a-9147a13f090b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0684)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 5)\n",
    "b = torch.randn(3, 5)\n",
    "l(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f128373d-c2fc-462b-839d-075e68b2846f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5887)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MSELoss()(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ce498ec-e0a8-4c91-a5b0-b52e75171b56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones([10]).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87887ee2-68a7-4959-9937-6639a7955845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
