{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import yaml\n",
    "\n",
    "from graph_generation import create_ER, dataset_nclass_ER, dataset_regression_ER\n",
    "from models import GCN\n",
    "from train import Trainer, Dataset\n",
    "from embedding import Embedding\n",
    "from experiments import experiment_embedding\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "device = torch.device('cuda')\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepdiff import DeepDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean connectivity for each node: 12.962 p=0.05\n",
      "Mean connectivity for each node: 77.661 p=0.3\n",
      "Labels values: {0, 1}\n"
     ]
    }
   ],
   "source": [
    "config = yaml.safe_load(open(\"configs.yml\"))\n",
    "dataset_grafi_nx, dataset_labels, list_p= dataset_nclass_ER(config)  # impostare ultimo neuron layer > 1 anche se cambia poi con la grid search\n",
    "#dataset_grafi_nx, dataset_labels = dataset_regression_ER(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "layers1 = [[1, 8, i] for i in range(2, 9, 6)]\n",
    "layers1_2 = [[1, 16, i] for i in range(2, 17, 2)]\n",
    "\n",
    "layers1_3 = [[1, 32, i] for i in range(2, 33, 2)]\n",
    "layers2 = [[1, i, i, 1] for i in range(2, 100, 12)]\n",
    "layers = layers1 + layers2\n",
    "\n",
    "num_nodes = [260, 280, 300, 320, 340, 360]\n",
    "\n",
    "epochs = [0, 1, 3, 10, 50, 5000]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#trials = list(itertools.product(learning_rates, layers, epochs))\n",
    "trials = list(itertools.product(layers1, layers1_2, layers1_3))\n",
    "print(len(trials))\n",
    "random.shuffle(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = layers1# + layers1_2 + layers1_3\n",
    "trials = [[1,32,32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def make_config(config, trials):\n",
    "    configs = []\n",
    "    #for lr, layer, num_nodes in trials:\n",
    "    for layer in trials:\n",
    "        #config['training']['learning_rate'] = lr\n",
    "        \n",
    "        config['model']['neurons_per_layer'] = layer\n",
    "        #config['training']['epochs'] = epoch\n",
    "        #config['graph_dataset']['Num_nodes'] = num_nodes\n",
    "        configs.append(copy.deepcopy(config))\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs2 = make_config(config, trials)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# carica il master config\n",
    "import json\n",
    "with open('configs_13-6-22.json', 'r') as fp:\n",
    "    configs = json.load(fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carica il master config\n",
    "with open('configs_21-6-22.pickle', 'rb') as handle:\n",
    "    configs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check su run già fatti\n",
    "actual_runs = []\n",
    "for i, c2 in enumerate(configs2):\n",
    "    cambi = True\n",
    "    for j, c in enumerate(configs):\n",
    "        ddiff = DeepDiff(c, c2, ignore_order=False)\n",
    "        cambi = ddiff.get('values_changed')\n",
    "        if not cambi:\n",
    "            print(f'salto i:{i} j:{j}')\n",
    "            break    \n",
    "    if cambi:\n",
    "        actual_runs.append(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(len(actual_runs), len(configs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0/1\n",
      "Batch Normalization included\n",
      "GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1, 32)\n",
      "    (1): GCNConv(32, 32)\n",
      "  )\n",
      "  (pools): ModuleList()\n",
      "  (batchnorms): ModuleList(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (leaky): LeakyReLU(negative_slope=0.03)\n",
      "  (lin): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [00:29<00:00, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo impiegato: 29.939348697662354\n",
      "runs/lr-0.01_epochs2500_bs100_neurons-1-32-32_21Jun_19-16-45\n",
      "Run training for 2500 epochs\n",
      "Epoch: 0\tTest loss: 0.0077365722921159535\n",
      "Epoch: 100\tTest loss: 1.0568221695292676e-08\n",
      "Epoch: 200\tTest loss: 3.5928352638671237e-10\n",
      "Epoch: 300\tTest loss: 0.0\n",
      "Epoch: 400\tTest loss: 0.0\n",
      "Epoch: 500\tTest loss: 0.0\n",
      "Epoch: 600\tTest loss: 0.0\n",
      "Epoch: 700\tTest loss: 0.0\n",
      "Epoch: 800\tTest loss: 0.0\n",
      "Epoch: 900\tTest loss: 0.0\n",
      "Epoch: 1000\tTest loss: 0.0\n",
      "Epoch: 1100\tTest loss: 0.0\n",
      "Epoch: 1200\tTest loss: 0.0\n",
      "Epoch: 1300\tTest loss: 0.0\n",
      "Epoch: 1400\tTest loss: 0.0\n",
      "Epoch: 1500\tTest loss: 0.0\n",
      "Epoch: 1600\tTest loss: 0.0\n",
      "Epoch: 1700\tTest loss: 0.0\n",
      "Epoch: 1800\tTest loss: 0.0\n",
      "Epoch: 1900\tTest loss: 0.0\n",
      "Epoch: 2000\tTest loss: 0.0\n",
      "Epoch: 2100\tTest loss: 0.0\n",
      "Epoch: 2200\tTest loss: 0.0\n",
      "Epoch: 2300\tTest loss: 0.0\n",
      "Epoch: 2400\tTest loss: 0.0\n",
      "Batch Normalization included\n",
      "GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(1, 32)\n",
      "    (1): GCNConv(32, 32)\n",
      "  )\n",
      "  (pools): ModuleList()\n",
      "  (batchnorms): ModuleList(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (leaky): LeakyReLU(negative_slope=0.03)\n",
      "  (lin): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "CrossEntropyLoss()\n",
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [00:33<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo impiegato: 33.82444715499878\n",
      "runs/lr-0.01_epochs2500_bs100_neurons-1-32-32_21Jun_19-21-28\n",
      "Run training for 2500 epochs\n",
      "Epoch: 0\tTest loss: 0.007848604851298862\n",
      "Epoch: 100\tTest loss: 1.5563433011480912e-10\n",
      "Epoch: 200\tTest loss: 1.4901160552385464e-11\n",
      "Epoch: 300\tTest loss: 1.4901160552385464e-11\n",
      "Epoch: 400\tTest loss: 2.1523897479151324e-11\n",
      "Epoch: 500\tTest loss: 0.0\n",
      "Epoch: 600\tTest loss: 0.0\n",
      "Epoch: 700\tTest loss: 0.0\n",
      "Epoch: 800\tTest loss: 0.0\n",
      "Epoch: 900\tTest loss: 0.0\n",
      "Epoch: 1000\tTest loss: 0.0\n",
      "Epoch: 1100\tTest loss: 0.0\n",
      "Epoch: 1200\tTest loss: 0.0\n",
      "Epoch: 1300\tTest loss: 0.0\n",
      "Epoch: 1400\tTest loss: 0.0\n",
      "Epoch: 1500\tTest loss: 0.0\n",
      "Epoch: 1600\tTest loss: 0.0\n",
      "Epoch: 1700\tTest loss: 0.0\n",
      "Epoch: 1800\tTest loss: 0.0\n",
      "Epoch: 1900\tTest loss: 0.0\n",
      "Epoch: 2000\tTest loss: 0.0\n",
      "Epoch: 2100\tTest loss: 0.0\n",
      "Epoch: 2200\tTest loss: 0.0\n",
      "Epoch: 2300\tTest loss: 0.0\n",
      "Epoch: 2400\tTest loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#regression: list_p = None,last parameter True\n",
    "# classificatrion: list_p, last par = False\n",
    "\n",
    "for i, c in enumerate(actual_runs):\n",
    "    print(f'Run {i}/{len(actual_runs)}')\n",
    "    for j in range(2):\n",
    "        embeddings, trainer, test_loss_list = experiment_embedding(c, dataset_grafi_nx, dataset_labels, list_p, False)\n",
    "        #corrs, error = embeddings.calc_correlation()\n",
    "        #c['correlations'] = corrs\n",
    "        #c['error'] = error \n",
    "        c['test_loss'] = test_loss_list\n",
    "        c['distance_of_means'] = embeddings.distance_of_means   # TODO: sarebbe il caso di mettere una gerarchia di chiavi anche per i risultati\n",
    "        #c[f'run_num'] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dati = actual_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config2df(dati):\n",
    "    df_data = pd.DataFrame(columns=['range_p', 'Num_nodes', 'Num_grafi_totali', 'learning_rate', 'batch_size', 'neurons_per_layer', 'correlations', 'error', 'test_loss', 'distance_of_means', 'batch_norm'])\n",
    "    df_data['range_p'] = [d['graph_dataset']['range_p'] for d in dati]\n",
    "    df_data['Num_nodes'] = [d['graph_dataset']['Num_nodes'] for d in dati]\n",
    "    df_data['Num_grafi_totali'] = [d['graph_dataset']['Num_grafi_totali'] for d in dati]\n",
    "    df_data['learning_rate'] = [d['training']['learning_rate'] for d in dati]\n",
    "    df_data['batch_size'] = [d['training']['batch_size'] for d in dati]\n",
    "    df_data['neurons_per_layer'] = [d['model']['neurons_per_layer'] for d in dati]\n",
    "    df_data['correlations'] = [d.get('correlations') for d in dati]\n",
    "    df_data['error'] = [d.get('error') for d in dati]\n",
    "    df_data['test_loss'] = [[round(l, 10) for l in d.get('test_loss') or []] for d in dati ]  # or [] : per il caso in cui d.get restituisca None\n",
    "    df_data['distance_of_means'] = [d.get('distance_of_means') for d in dati]  # caso della classification\n",
    "    #df_data['run_num'] = [d.get('run_num') for d in dati]\n",
    "    df_data['batch_norm'] = [d['model']['put_batchnorm'] for d in dati]\n",
    "    df_data['num_epochs'] = [d['training']['epochs'] for d in dati]\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correggo il dataframe esistente aggiungendo i campi che mi viene in mente di aggiungere nel tempo\n",
    "file = \"df_configs_class.csv\" #\"df_configs.csv\" #\n",
    "df_data = pd.read_csv(file)  \n",
    "df_data['num_epochs'] = 5000\n",
    "df_data.to_csv(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = config2df(dati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>range_p</th>\n",
       "      <th>Num_nodes</th>\n",
       "      <th>Num_grafi_totali</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>neurons_per_layer</th>\n",
       "      <th>correlations</th>\n",
       "      <th>error</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>distance_of_means</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>num_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.05, 0.6]</td>\n",
       "      <td>260</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>[1, 32, 32]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0078486049, 0.0077183223, 0.0077000621, 0.0...</td>\n",
       "      <td>15.375359</td>\n",
       "      <td>True</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       range_p  Num_nodes  Num_grafi_totali  learning_rate  batch_size  \\\n",
       "0  [0.05, 0.6]        260              1000           0.01         100   \n",
       "\n",
       "  neurons_per_layer correlations error  \\\n",
       "0       [1, 32, 32]         None  None   \n",
       "\n",
       "                                           test_loss  distance_of_means  \\\n",
       "0  [0.0078486049, 0.0077183223, 0.0077000621, 0.0...          15.375359   \n",
       "\n",
       "   batch_norm  num_epochs  \n",
       "0        True        2500  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "outfile = \"df_configs_class.csv\" #\"df_configs.csv\" #\n",
    "df_data.to_csv(outfile, index=False, mode='a', header=not os.path.exists(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggiungo al master config\n",
    "configs.extend(actual_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in configs:\n",
    "    if c.get('test_loss'):\n",
    "        del c['test_loss']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "with open('configs_21-6-22.json', 'w') as f:\n",
    "    json.dump(configs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs_21-6-22.pickle', 'wb') as handle:\n",
    "    pickle.dump(configs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Geometric",
   "language": "python",
   "name": "pytorch-geom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
