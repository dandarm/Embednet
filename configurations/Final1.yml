graph_dataset:
    ERmodel: True
    regular: False
    confmodel: False
    sbm: False
    const_degree_dist: False
    real_dataset: False
    real_data_name:  'BACI'  #'REDDIT-BINARY'   #
    continuous_p: False
    random_node_feat: False
    list_p: [0.01] #, 0.2, 0.15, 0.1] #, 0.01]
    range_p: [0.01, 0.5]
    community_probs: [[[0.25, 0.01], [0.01, 0.1]], [[0.4, 0.01], [0.01, 0.3]], [[0.4, 0.01], [0.01, 0.1]]]  # num nodi * num classi * num comunit√†
    Num_nodes: [300] #, 200, 200, 200] # , 200]
    Num_grafi_per_tipo: 20
    Num_grafi_totali: 1000
    list_degree: [7]
    list_exponents: [-2.5]  #, -2.5, -2.8, -3.0, -3.2]  #, -3.5, -3.7]
    #max_degree_const_dist: 50
    
    
    
training:
    #mode1 = 'classification', CrossEntropyLoss, '1-hot', 'n_class'
    #mode2 = 'classification', BCELoss, '{0,1}', 1
    #mode3 = 'regression', MSELoss, 'p', 1}
    mode: 'mode1'
    learning_rate: 0.0001
    epochs: 100000
    epochs_list_points: 40
    test_loss_every_epochs: 200
    batch_size: 1000
    percentage_train: 0.70
    earlystop_patience: 500
    epochs_model_checkpoint: [-1]
    save_best_model: True
    every_epoch_embedding: True
    shuffle_dataset: True
    calculate_metrics: True
    optimizer: 'ADAMW'
    #weigths4unbalanced_dataset: True
    loss: 'BCELoss' # MSELoss  "MSELoss_ww"  "BCELoss_ww"
    repetitions: 0

plot:
    x_axis_log: True
    plot_model: False
    plot_embeddings: True
    
model:
    last_layer_dense: False
    GCNneurons_per_layer: [1, 32, 32, 32]
    neurons_last_linear: [32, 32]
    freezeGCNlayers: False
    only_encoder: False
    autoencoder: False
    autoencoder_confmodel: True
    autoencoder_graph_ae: False
    autoencoder_mlpdecoder: False
    autoencoder_fullMLP: False
    autoencoder_degseq: False
    autoencoder_MLPCM: False
    autoencoder_fullMLP_CM: False
    put_batchnorm: False
    put_graphnorm: False
    final_pool_aggregator: True
    put_dropout: False
    node_features_dim: 1
    init_weights:  'xavier_normal' #  'kaiming_uniform'  'normal'
    activation: 'ELU'   # LeakyRELU , Hardtanh, 'RELU' Tanh'
    last_layer_activation: 'RELU'  #'5*Softsign'    #
    normalized_adj: False
    my_normalization_adj: True
    
graph_ae_model:
    num_kernels: 2
    depth: 3
    comp_rate: 0.8
    neurons_per_layer: [1,32,32,32]

mlp_ae_model:
    neurons_per_layer: [16, 16]
    activation: 'ELU'
    last_layer_activation: 'Sigmoid'
    
logging:
    train_step_print: 1000
    
device: 'gpu'